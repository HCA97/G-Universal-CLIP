{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e18b1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "\n",
    "# DATALOADER\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# BUILDING MODEL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# TRAINING\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import faiss\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# OTHER STUFF\n",
    "import timm\n",
    "from transformers import (get_linear_schedule_with_warmup, \n",
    "                          get_cosine_schedule_with_warmup, \n",
    "                          get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "                          get_constant_schedule_with_warmup)\n",
    "import gc\n",
    "import transformers\n",
    "from transformers import CLIPProcessor, CLIPVisionModel,  CLIPVisionConfig\n",
    "from pytorch_metric_learning import losses\n",
    "import open_clip\n",
    "\n",
    "# UTILS\n",
    "import utilities\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2484dfa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3366087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RN50', 'openai'),\n",
       " ('RN50', 'yfcc15m'),\n",
       " ('RN50', 'cc12m'),\n",
       " ('RN50-quickgelu', 'openai'),\n",
       " ('RN50-quickgelu', 'yfcc15m'),\n",
       " ('RN50-quickgelu', 'cc12m'),\n",
       " ('RN101', 'openai'),\n",
       " ('RN101', 'yfcc15m'),\n",
       " ('RN101-quickgelu', 'openai'),\n",
       " ('RN101-quickgelu', 'yfcc15m'),\n",
       " ('RN50x4', 'openai'),\n",
       " ('RN50x16', 'openai'),\n",
       " ('RN50x64', 'openai'),\n",
       " ('ViT-B-32', 'openai'),\n",
       " ('ViT-B-32', 'laion400m_e31'),\n",
       " ('ViT-B-32', 'laion400m_e32'),\n",
       " ('ViT-B-32', 'laion2b_e16'),\n",
       " ('ViT-B-32', 'laion2b_s34b_b79k'),\n",
       " ('ViT-B-32-quickgelu', 'openai'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e31'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e32'),\n",
       " ('ViT-B-16', 'openai'),\n",
       " ('ViT-B-16', 'laion400m_e31'),\n",
       " ('ViT-B-16', 'laion400m_e32'),\n",
       " ('ViT-B-16', 'laion2b_s34b_b88k'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e31'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'openai'),\n",
       " ('ViT-L-14', 'laion400m_e31'),\n",
       " ('ViT-L-14', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'laion2b_s32b_b82k'),\n",
       " ('ViT-L-14-336', 'openai'),\n",
       " ('ViT-H-14', 'laion2b_s32b_b79k'),\n",
       " ('ViT-g-14', 'laion2b_s12b_b42k'),\n",
       " ('ViT-bigG-14', 'laion2b_s39b_b160k'),\n",
       " ('roberta-ViT-B-32', 'laion2b_s12b_b32k'),\n",
       " ('xlm-roberta-base-ViT-B-32', 'laion5b_s13b_b90k'),\n",
       " ('xlm-roberta-large-ViT-H-14', 'frozen_laion5b_s13b_b90k'),\n",
       " ('convnext_base', 'laion400m_s13b_b51k'),\n",
       " ('convnext_base_w', 'laion2b_s13b_b82k'),\n",
       " ('convnext_base_w', 'laion2b_s13b_b82k_augreg'),\n",
       " ('convnext_base_w', 'laion_aesthetic_s13b_b82k'),\n",
       " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k'),\n",
       " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k_augreg'),\n",
       " ('convnext_large_d', 'laion2b_s26b_b102k_augreg'),\n",
       " ('coca_ViT-B-32', 'laion2b_s13b_b90k'),\n",
       " ('coca_ViT-B-32', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
       " ('coca_ViT-L-14', 'laion2b_s13b_b90k'),\n",
       " ('coca_ViT-L-14', 'mscoco_finetuned_laion2b_s13b_b90k')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "191acbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name = 'ViT-H-14' \n",
    "    model_data = 'laion2b_s32b_b79k'\n",
    "    samples_per_class = 50\n",
    "    n_classes = 0\n",
    "    min_samples = 4\n",
    "    image_size = 224 \n",
    "    hidden_layer = 1024\n",
    "    seed = 5\n",
    "    workers = 6\n",
    "    train_batch_size = 8\n",
    "    valid_batch_size = 32 \n",
    "    emb_size = 512\n",
    "    vit_bb_lr = {'10': 1.25e-6, '20': 2.5e-6, '26': 5e-6, '32': 10e-6} \n",
    "    vit_bb_wd = 1e-3\n",
    "    hd_lr = 3e-4\n",
    "    hd_wd = 1e-5\n",
    "    autocast = True\n",
    "    n_warmup_steps = 1000\n",
    "    n_epochs = 2\n",
    "    device = torch.device('cuda')\n",
    "    s=30.\n",
    "    m=.45\n",
    "    m_min=.05\n",
    "    acc_steps = 4\n",
    "    global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "585726e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "def1556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "utilities.set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44aa8e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5301/3957615659.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for cls in tqdm(set(df_g['class'])):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b712cbb43149389d6427a69743396b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9691 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv_path = '../products-10k/train.csv'\n",
    "dataset_path = '../products-10k/train'\n",
    "\n",
    "def get_data(csv_path, dataset_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df_g = df.groupby('class', group_keys=True).apply(lambda x: x)\n",
    "\n",
    "    training_samples = []\n",
    "    num_classes = 0\n",
    "    new_cls_id = 0\n",
    "    value_counts = []\n",
    "    for cls in tqdm(set(df_g['class'])):\n",
    "        paths = list(df_g.name[df_g['class'] == cls])\n",
    "        if len(paths) >= CFG.min_samples:\n",
    "            num_classes += 1\n",
    "\n",
    "            random.shuffle(paths)\n",
    "\n",
    "            paths_ = [ \n",
    "                (os.path.join(dataset_path, p), new_cls_id) \n",
    "                for p in paths[:CFG.samples_per_class]\n",
    "            ]\n",
    "            \n",
    "            # some classes will be neglect it\n",
    "            new_cls_id += 1\n",
    "            \n",
    "            value_counts.append(len(paths_))\n",
    "            training_samples.extend(paths_)\n",
    "            \n",
    "    return training_samples, value_counts, num_classes\n",
    "\n",
    "CFG.samples_per_class = 50\n",
    "data_train, value_counts_train, num_classes_train = get_data('../products-10k/train.csv', '../products-10k/train')\n",
    "\n",
    "value_counts = np.array(value_counts_train)\n",
    "CFG.n_classes = num_classes_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caac8f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139875, 9004)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train), CFG.n_classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12a24703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, hidden_size, k=3, skip_embedding=False):\n",
    "        super(Head, self).__init__()\n",
    "        \n",
    "        if not skip_embedding:\n",
    "            self.emb = nn.Linear(hidden_size, CFG.emb_size, bias=False)\n",
    "            self.dropout = utilities.Multisample_Dropout()\n",
    "            self.arc = utilities.ArcMarginProduct_subcenter(CFG.emb_size, CFG.n_classes, k)\n",
    "        else:\n",
    "            self.arc = utilities.ArcMarginProduct_subcenter(hidden_size, CFG.n_classes, k)\n",
    "        self.skip_embedding = skip_embedding\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.skip_embedding:\n",
    "            embeddings = self.dropout(x, self.emb)\n",
    "        else:\n",
    "            embeddings = x\n",
    "        output = self.arc(embeddings)\n",
    "\n",
    "        return output, F.normalize(embeddings)\n",
    "    \n",
    "class HeadV3(nn.Module):\n",
    "    def __init__(self, hidden_size, k=3):\n",
    "        super(Head, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Linear(hidden_size, CFG.emb_size, bias=False)\n",
    "        self.dropout = nn.Dropout1d(0.2)\n",
    "        self.skip_embedding = skip_embedding\n",
    "        self.arc = utilities.ArcMarginProduct_subcenter(CFG.emb_size, CFG.n_classes, k)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x = self.emb(x)\n",
    "        output = self.arc(x)\n",
    "        return output, F.normalize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e35b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vit_backbone, head_size, version='v1', k=3):\n",
    "        super(Model, self).__init__()\n",
    "        if version == 'v1':\n",
    "            self.head = Head(head_size, k)\n",
    "        elif version == 'v2':\n",
    "            self.head = Head(head_size, k, skip_embedding=True)\n",
    "        elif version == 'v3':\n",
    "            self.head = HeadV3(head_size, k)\n",
    "        else:\n",
    "            self.head = Head(head_size, k)\n",
    "        \n",
    "        self.encoder = vit_backbone.visual\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.head(x)\n",
    "\n",
    "    def get_parameters(self):\n",
    "\n",
    "        parameter_settings = [] \n",
    "        parameter_settings.extend(\n",
    "            self.get_parameter_section(\n",
    "                [(n, p) for n, p in self.encoder.named_parameters()], \n",
    "                lr=CFG.vit_bb_lr, \n",
    "                wd=CFG.vit_bb_wd\n",
    "            )\n",
    "        ) \n",
    "\n",
    "        parameter_settings.extend(\n",
    "            self.get_parameter_section(\n",
    "                [(n, p) for n, p in self.head.named_parameters()], \n",
    "                lr=CFG.hd_lr, \n",
    "                wd=CFG.hd_wd\n",
    "            )\n",
    "        ) \n",
    "\n",
    "        return parameter_settings\n",
    "\n",
    "    def get_parameter_section(self, parameters, lr=None, wd=None): \n",
    "        parameter_settings = []\n",
    "\n",
    "\n",
    "        lr_is_dict = isinstance(lr, dict)\n",
    "        wd_is_dict = isinstance(wd, dict)\n",
    "\n",
    "        layer_no = None\n",
    "        for no, (n,p) in enumerate(parameters):\n",
    "            \n",
    "            for split in n.split('.'):\n",
    "                if split.isnumeric():\n",
    "                    layer_no = int(split)\n",
    "            \n",
    "            if not layer_no:\n",
    "                layer_no = 0\n",
    "            \n",
    "            if lr_is_dict:\n",
    "                for k,v in lr.items():\n",
    "                    if layer_no < int(k):\n",
    "                        temp_lr = v\n",
    "                        break\n",
    "            else:\n",
    "                temp_lr = lr\n",
    "\n",
    "            if wd_is_dict:\n",
    "                for k,v in wd.items():\n",
    "                    if layer_no < int(k):\n",
    "                        temp_wd = v\n",
    "                        break\n",
    "            else:\n",
    "                temp_wd = wd\n",
    "\n",
    "            weight_decay = 0.0 if 'bias' in n else temp_wd\n",
    "\n",
    "            parameter_setting = {\"params\" : p, \"lr\" : temp_lr, \"weight_decay\" : temp_wd}\n",
    "\n",
    "            parameter_settings.append(parameter_setting)\n",
    "\n",
    "            #print(f'no {no} | params {n} | lr {temp_lr} | weight_decay {weight_decay} | requires_grad {p.requires_grad}')\n",
    "\n",
    "        return parameter_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5281af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ArcFace_criterion(logits_m, target, margins):\n",
    "    arc = utilities.ArcFaceLossAdaptiveMargin(margins=margins, s=CFG.s)\n",
    "    loss_m = arc(logits_m, target, CFG.n_classes)\n",
    "    return loss_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6e7f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, scaler, scheduler, epoch):\n",
    "    model.train()\n",
    "    loss_metrics = utilities.AverageMeter()\n",
    "    criterion = ArcFace_criterion\n",
    "\n",
    "    tmp = np.sqrt(1 / np.sqrt(value_counts))\n",
    "    margins = (tmp - tmp.min()) / (tmp.max() - tmp.min()) * CFG.m + CFG.m_min\n",
    "        \n",
    "    bar = tqdm(train_loader)\n",
    "    for step, data in enumerate(bar):\n",
    "        step += 1\n",
    "        images = data['images'].to(CFG.device, dtype=torch.float)\n",
    "        labels = data['labels'].to(CFG.device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.autocast):\n",
    "            outputs, features = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels, margins)\n",
    "        loss_metrics.update(loss.item(), batch_size)\n",
    "        loss = loss / CFG.acc_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if step % CFG.acc_steps == 0 or step == len(bar):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            CFG.global_step += 1\n",
    "            \n",
    "        lrs = utilities.get_lr_groups(optimizer.param_groups)\n",
    "\n",
    "        loss_avg = loss_metrics.avg\n",
    "\n",
    "        bar.set_postfix(loss=loss_avg, epoch=epoch, lrs=lrs, step=CFG.global_step)\n",
    "\n",
    "@torch.no_grad()\n",
    "def val(model, valid_loader):\n",
    "    model.eval() \n",
    "\n",
    "    all_embeddings = []\n",
    "    all_labels = [] \n",
    "\n",
    "    for data in tqdm(valid_loader):\n",
    "        images = data['images'].to(CFG.device, dtype=torch.float)\n",
    "        labels = data['labels'].to(CFG.device)\n",
    "\n",
    "        _, embeddings = model(images)\n",
    "\n",
    "        all_embeddings.append(embeddings.detach().cpu().numpy())\n",
    "        all_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "    all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    return all_embeddings, all_labels\n",
    "\n",
    "def training(train_loader, gallery_loader, query_loader, experiment_folder, version='v1', k=3):\n",
    "    \n",
    "    os.makedirs(experiment_folder, exist_ok=True)\n",
    "    \n",
    "    backbone, _, _ = open_clip.create_model_and_transforms(CFG.model_name, CFG.model_data)\n",
    "    model = Model(backbone, CFG.hidden_layer, version, k).to(CFG.device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.get_parameters())\n",
    " \n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.autocast)\n",
    "\n",
    "    steps_per_epoch = math.ceil(len(train_loader) / CFG.acc_steps)\n",
    "\n",
    "    num_training_steps = math.ceil(CFG.n_epochs * steps_per_epoch)\n",
    "\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                num_training_steps=num_training_steps,\n",
    "                                                num_warmup_steps=CFG.n_warmup_steps)  \n",
    "\n",
    "    best_score = 0\n",
    "    best_updated_ = 0\n",
    "    CFG.global_step = 0                   \n",
    "    for epoch in range(math.ceil(CFG.n_epochs)):\n",
    "        print(f'starting epoch {epoch}')\n",
    "\n",
    "        # train of product-10k\n",
    "        train(model, train_loader, optimizer, scaler, scheduler, epoch)\n",
    "\n",
    "        # aicrowd test data\n",
    "        print('gallery embeddings')\n",
    "        embeddings_gallery, labels_gallery = val(model, gallery_loader)\n",
    "        print('query embeddings')\n",
    "        embeddings_query, labels_query = val(model, query_loader)\n",
    "\n",
    "        # idk why it is needed\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "\n",
    "        # calculate validation score\n",
    "        _, indices = utilities.get_similiarity_l2(embeddings_gallery, embeddings_query, 1000)\n",
    "\n",
    "\n",
    "        indices = indices.tolist()\n",
    "        labels_gallery = labels_gallery.tolist()\n",
    "        labels_query = labels_query.tolist()\n",
    "\n",
    "        preds = utilities.convert_indices_to_labels(indices, labels_gallery)\n",
    "        score = utilities.map_per_set(labels_query, preds)\n",
    "        print('validation score', score)\n",
    "\n",
    "        # save model\n",
    "        torch.save({\n",
    "                'model_state_dict': model.state_dict()\n",
    "                }, f'{experiment_folder}/model_epoch_{epoch+1}_mAP3_{score:.2f}.pt')\n",
    "\n",
    "        # save the best model\n",
    "        if score > best_score:\n",
    "            best_updated_ = 0\n",
    "            best_score = score\n",
    "\n",
    "        best_updated_ += 1\n",
    "\n",
    "        if best_updated_ >= 3:\n",
    "            print('no improvement done training....')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ece494b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "\n",
    "def read_img(img_path, is_gray=False):\n",
    "    mode = cv2.IMREAD_COLOR if not is_gray else cv2.IMREAD_GRAYSCALE\n",
    "    img = cv2.imread(img_path, mode)\n",
    "    if not is_gray:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "def get_final_transform():  \n",
    "    final_transform = T.Compose([\n",
    "            T.Resize(\n",
    "                size=(CFG.image_size, CFG.image_size), \n",
    "                interpolation=T.InterpolationMode.BICUBIC,\n",
    "                antialias=True),\n",
    "            T.ToTensor(), \n",
    "            T.Normalize(\n",
    "                mean=(0.48145466, 0.4578275, 0.40821073), \n",
    "                std=(0.26862954, 0.26130258, 0.27577711)\n",
    "            )\n",
    "        ])\n",
    "    return final_transform\n",
    "\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data, \n",
    "                 transform=None, \n",
    "                 final_transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.final_transform = final_transform\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "       \n",
    "        img = read_img(self.data[idx][0])            \n",
    "        \n",
    "        if self.transform is not None:\n",
    "            if isinstance(self.transform, A.Compose):\n",
    "                img = self.transform(image=img)['image']\n",
    "            else:\n",
    "                img = self.transform(img)\n",
    "        \n",
    "        if self.final_transform is not None:\n",
    "            if isinstance(img, np.ndarray):\n",
    "                img =  Image.fromarray(img)\n",
    "            img = self.final_transform(img)\n",
    "            \n",
    "        product_id = self.data[idx][1]\n",
    "        return {\"images\": img, \"labels\": product_id}\n",
    "    \n",
    "def get_product_10k_dataloader(data_train, data_aug='image_net'):\n",
    "    \n",
    "    transform = None\n",
    "    if data_aug == 'image_net':\n",
    "        transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.AutoAugment(T.AutoAugmentPolicy.IMAGENET)\n",
    "        ])\n",
    "    elif data_aug == 'cut_out':\n",
    "        aug8p3 = A.OneOf([\n",
    "            A.Sharpen(p=0.3),\n",
    "            A.ToGray(p=0.3),\n",
    "            A.CLAHE(p=0.3),\n",
    "        ], p=0.5)\n",
    "\n",
    "        transform = A.Compose([\n",
    "            A.ShiftScaleRotate(rotate_limit=15, scale_limit=0.1, border_mode=cv2.BORDER_REFLECT, p=0.5),\n",
    "            A.Resize(CFG.image_size, CFG.image_size),\n",
    "            aug8p3,\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1)\n",
    "        ])\n",
    "    \n",
    "    elif data_aug == 'happy_whale':        \n",
    "        transform = A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ImageCompression(quality_lower=99, quality_upper=100),\n",
    "            A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=0.7),\n",
    "            A.Resize(CFG.image_size, CFG.image_size),\n",
    "            A.Cutout(max_h_size=int(CFG.image_size * 0.4), \n",
    "                     max_w_size=int(CFG.image_size * 0.4), \n",
    "                     num_holes=1, p=0.5),\n",
    "        ])\n",
    "    elif data_aug == 'clip':\n",
    "        transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.RandomResizedCrop(\n",
    "                size=(224, 224), \n",
    "                scale=(0.9, 1.0), \n",
    "                ratio=(0.75, 1.3333), \n",
    "                interpolation=T.InterpolationMode.BICUBIC,\n",
    "                antialias=True\n",
    "            )\n",
    "        ])\n",
    "    elif data_aug == 'clip+image_net':\n",
    "        transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.AutoAugment(T.AutoAugmentPolicy.IMAGENET),\n",
    "            T.RandomResizedCrop(\n",
    "                size=(224, 224), \n",
    "                scale=(0.9, 1.0), \n",
    "                ratio=(0.75, 1.3333), \n",
    "                interpolation=T.InterpolationMode.BICUBIC,\n",
    "                antialias=True\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    final_transform = get_final_transform()\n",
    "    train_dataset = ProductDataset(data_train, \n",
    "                                   transform, \n",
    "                                   final_transform)\n",
    "    train_loader = DataLoader(train_dataset, \n",
    "                              batch_size = CFG.train_batch_size, \n",
    "                              num_workers=CFG.workers, \n",
    "                              shuffle=True, \n",
    "                              drop_last=True)\n",
    "    print(f'Training Data -> Dataset Length ({len(train_dataset)})')\n",
    "    return train_loader\n",
    "\n",
    "def aicrowd_data_loader(csv_path, img_dir='../development_test_data'):\n",
    "    df_g = pd.read_csv(csv_path)\n",
    "    df_g_ = df_g[['img_path', 'product_id']]\n",
    "    df_g_['img_path'] = df_g_.apply(lambda x: img_dir + '/' + x['img_path'], axis=1)\n",
    "    data_ = np.array(df_g_).tolist()\n",
    "    \n",
    "    final_transform = get_final_transform()\n",
    "    dataset = ProductDataset(data_, None, final_transform)\n",
    "    data_loader = DataLoader(dataset, \n",
    "                             batch_size = CFG.valid_batch_size, \n",
    "                             num_workers=CFG.workers, \n",
    "                             shuffle=False, \n",
    "                             drop_last=False)\n",
    "    \n",
    "    print(f'{csv_path} -> Dataset Length ({len(dataset)})')\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "548890dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../development_test_data/gallery.csv -> Dataset Length (1067)\n",
      "../development_test_data/queries.csv -> Dataset Length (1935)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5301/2861557302.py:126: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_g_['img_path'] = df_g_.apply(lambda x: img_dir + '/' + x['img_path'], axis=1)\n",
      "/tmp/ipykernel_5301/2861557302.py:126: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_g_['img_path'] = df_g_.apply(lambda x: img_dir + '/' + x['img_path'], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# aicrowd datasets\n",
    "gallery_loader = aicrowd_data_loader('../development_test_data/gallery.csv') \n",
    "query_loader = aicrowd_data_loader('../development_test_data/queries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae01314b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data -> Dataset Length (139875)\n",
      "starting epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5301/498419007.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  bar = tqdm(train_loader)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca04fb78b50460292baa34d97ee541e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cemmi/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gallery embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5301/498419007.py:44: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for data in tqdm(valid_loader):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2017a3a40342408736ae90fa4c4e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e04f9b829da4717a00af3a71a378f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing indices...\n",
      "Finished processing indices, took 0.20327186584472656s\n",
      "validation score 0.5396640826873385\n",
      "starting epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad44f8ca94b44dfa2f698117868b54a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 3\n",
    "version = 'v2'\n",
    "for data_aug in ['clip', 'image_net', 'clip+image_net']:\n",
    "    train_loader = get_product_10k_dataloader(data_train, data_aug)\n",
    "    experiment_folder = f'my_experiments/{CFG.model_name}-{CFG.model_data}-{str(data_aug)}-{str(version)}-product-10k-Arcface(k={str(k)})-Epoch({str(CFG.n_epochs)})'\n",
    "    training(train_loader, gallery_loader, query_loader, experiment_folder, version=version, k=k)\n",
    "    # idk why it is needed\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33db0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
