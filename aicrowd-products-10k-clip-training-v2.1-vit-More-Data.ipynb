{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e18b1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "\n",
    "# DATALOADER\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# BUILDING MODEL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# TRAINING\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import faiss\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# OTHER STUFF\n",
    "import timm\n",
    "from transformers import (get_linear_schedule_with_warmup, \n",
    "                          get_constant_schedule,\n",
    "                          get_cosine_schedule_with_warmup, \n",
    "                          get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "                          get_constant_schedule_with_warmup)\n",
    "import gc\n",
    "import transformers\n",
    "from transformers import CLIPProcessor, CLIPVisionModel,  CLIPVisionConfig\n",
    "from pytorch_metric_learning import losses\n",
    "import open_clip\n",
    "\n",
    "# UTILS\n",
    "import utilities\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2484dfa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3366087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RN50', 'openai'),\n",
       " ('RN50', 'yfcc15m'),\n",
       " ('RN50', 'cc12m'),\n",
       " ('RN50-quickgelu', 'openai'),\n",
       " ('RN50-quickgelu', 'yfcc15m'),\n",
       " ('RN50-quickgelu', 'cc12m'),\n",
       " ('RN101', 'openai'),\n",
       " ('RN101', 'yfcc15m'),\n",
       " ('RN101-quickgelu', 'openai'),\n",
       " ('RN101-quickgelu', 'yfcc15m'),\n",
       " ('RN50x4', 'openai'),\n",
       " ('RN50x16', 'openai'),\n",
       " ('RN50x64', 'openai'),\n",
       " ('ViT-B-32', 'openai'),\n",
       " ('ViT-B-32', 'laion400m_e31'),\n",
       " ('ViT-B-32', 'laion400m_e32'),\n",
       " ('ViT-B-32', 'laion2b_e16'),\n",
       " ('ViT-B-32', 'laion2b_s34b_b79k'),\n",
       " ('ViT-B-32-quickgelu', 'openai'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e31'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e32'),\n",
       " ('ViT-B-16', 'openai'),\n",
       " ('ViT-B-16', 'laion400m_e31'),\n",
       " ('ViT-B-16', 'laion400m_e32'),\n",
       " ('ViT-B-16', 'laion2b_s34b_b88k'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e31'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'openai'),\n",
       " ('ViT-L-14', 'laion400m_e31'),\n",
       " ('ViT-L-14', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'laion2b_s32b_b82k'),\n",
       " ('ViT-L-14-336', 'openai'),\n",
       " ('ViT-H-14', 'laion2b_s32b_b79k'),\n",
       " ('ViT-g-14', 'laion2b_s12b_b42k'),\n",
       " ('ViT-bigG-14', 'laion2b_s39b_b160k'),\n",
       " ('roberta-ViT-B-32', 'laion2b_s12b_b32k'),\n",
       " ('xlm-roberta-base-ViT-B-32', 'laion5b_s13b_b90k'),\n",
       " ('xlm-roberta-large-ViT-H-14', 'frozen_laion5b_s13b_b90k'),\n",
       " ('convnext_base', 'laion400m_s13b_b51k'),\n",
       " ('convnext_base_w', 'laion2b_s13b_b82k'),\n",
       " ('convnext_base_w', 'laion2b_s13b_b82k_augreg'),\n",
       " ('convnext_base_w', 'laion_aesthetic_s13b_b82k'),\n",
       " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k'),\n",
       " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k_augreg'),\n",
       " ('convnext_large_d', 'laion2b_s26b_b102k_augreg'),\n",
       " ('coca_ViT-B-32', 'laion2b_s13b_b90k'),\n",
       " ('coca_ViT-B-32', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
       " ('coca_ViT-L-14', 'laion2b_s13b_b90k'),\n",
       " ('coca_ViT-L-14', 'mscoco_finetuned_laion2b_s13b_b90k')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "191acbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name = 'ViT-H-14' \n",
    "    model_data = 'laion2b_s32b_b79k'\n",
    "    samples_per_class = 50\n",
    "    n_classes = 0\n",
    "    min_samples = 4\n",
    "    image_size = 224 \n",
    "    hidden_layer = 1024\n",
    "    seed = 5\n",
    "    workers = 12\n",
    "    train_batch_size = 8\n",
    "    valid_batch_size = 32 \n",
    "    emb_size = 512\n",
    "    vit_bb_lr = {'10': 1.25e-6, '20': 2.5e-6, '26': 5e-6, '32': 10e-6} \n",
    "    vit_bb_wd = 1e-3\n",
    "    hd_lr = 3e-4\n",
    "    hd_wd = 1e-5\n",
    "    autocast = True\n",
    "    n_warmup_steps = 1000\n",
    "    n_epochs = 10\n",
    "    device = torch.device('cuda')\n",
    "    s=30.\n",
    "    m=.45\n",
    "    m_min=.05\n",
    "    acc_steps = 4\n",
    "    global_step = 0\n",
    "    reduce_lr = 0.1\n",
    "    crit = 'ce'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "585726e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "def1556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "utilities.set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44aa8e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31657/2861725851.py:38: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for group in tqdm(set(df_g['label_group'])):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa766b03fb2498ebcea503dcdb71d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11008 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31657/2861725851.py:57: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for group in tqdm(set(df_g['id'])):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0013ea3e8a44aa0a7bafc7a781e5d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14752 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31657/2861725851.py:93: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for group in tqdm(set(df_g['class'])):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa51f1afcd84a008b9a6ac43322538c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9691 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# used for training\n",
    "training_samples = []\n",
    "values_counts = []\n",
    "num_classes = 0\n",
    "\n",
    "# H&M\n",
    "files = glob.glob(\"../H&M/images/*/*\")\n",
    "file_paths = dict((os.path.splitext(os.path.split(f)[-1])[0], f) for f in files)\n",
    "\n",
    "df = pd.read_csv('../H&M/articles.csv', \n",
    "                 usecols=['article_id', 'product_code'],\n",
    "                 dtype={'article_id': str, 'product_code': str})\n",
    "\n",
    "groupped_products = {}\n",
    "for index, row in df.iterrows():\n",
    "    v = groupped_products.get(row['product_code'], [])\n",
    "    f = file_paths.get(row['article_id'])\n",
    "    if f:\n",
    "        groupped_products[row['product_code']] = v + [f]\n",
    "\n",
    "\n",
    "for key, value in groupped_products.items():\n",
    "    if len(value) >= CFG.min_samples:\n",
    "        paths = value[:CFG.samples_per_class]\n",
    "        \n",
    "        values_counts.append(len(paths))\n",
    "        training_samples.extend([\n",
    "            (p, num_classes) for p in paths\n",
    "        ])\n",
    "        num_classes += 1\n",
    "        \n",
    "# Shoppee\n",
    "df = pd.read_csv('../shopee/train.csv')\n",
    "df = df.drop_duplicates(subset='image_phash', keep=\"last\")\n",
    "df_g = df.groupby('label_group', group_keys=True).apply(lambda x: x)\n",
    "\n",
    "\n",
    "for group in tqdm(set(df_g['label_group'])):\n",
    "    names = list(df_g.image[df_g['label_group'] == group])\n",
    "    if len(names) >= CFG.min_samples:\n",
    "        paths = [\n",
    "            os.path.join('../shopee/train_images', name) for name in names[:CFG.samples_per_class]\n",
    "        ]\n",
    "        \n",
    "\n",
    "        values_counts.append(len(paths))\n",
    "        training_samples.extend([\n",
    "            (p, num_classes) for p in paths\n",
    "        ])\n",
    "\n",
    "        num_classes += 1\n",
    "        \n",
    "# AMAZON-dataset\n",
    "df = pd.read_csv('../amazon_dataset_1/amazon_data_set_sample_15000_2.csv')\n",
    "df_g = df.groupby('id', group_keys=True).apply(lambda x: x)\n",
    "\n",
    "for group in tqdm(set(df_g['id'])):\n",
    "    names = list(df_g.path[df_g['id'] == group])\n",
    "    \n",
    "    if len(names) >= CFG.min_samples:\n",
    "        paths = [\n",
    "            os.path.join('../amazon_dataset_1', name) for name in names[:CFG.samples_per_class]\n",
    "        ]\n",
    "\n",
    "        values_counts.append(len(paths))\n",
    "        training_samples.extend([\n",
    "            (p, num_classes) for p in paths\n",
    "        ])\n",
    "        \n",
    "        num_classes += 1\n",
    "\n",
    "# Product-10k\n",
    "df = pd.read_csv('../products-10k/train.csv')\n",
    "df_g = df.groupby('class', group_keys=True).apply(lambda x: x)\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('../products-10k/train.csv')\n",
    "train_df['path'] = train_df.apply(lambda x: '../products-10k/train' + '/' + x['name'], axis=1)\n",
    "\n",
    "\n",
    "# remove ../products-10k/test/9397815.jpg from the list!\n",
    "test_df = pd.read_csv('../products-10k/test_kaggletest.csv')\n",
    "test_df = test_df.drop(test_df[test_df.name == '9397815.jpg'].index) # smt wrong with this img\n",
    "test_df['path'] = test_df.apply(lambda x: '../products-10k/test' + '/' + x['name'], axis=1)\n",
    "\n",
    "df = pd.concat([\n",
    "    test_df[['class','path']],\n",
    "    train_df[['class', 'path']]\n",
    "])\n",
    "df_g = df.groupby('class', group_keys=True).apply(lambda x: x)\n",
    "\n",
    "\n",
    "for group in tqdm(set(df_g['class'])):\n",
    "    names = list(df_g.path[df_g['class'] == group])\n",
    "    if len(names) >= CFG.min_samples:\n",
    "        paths = [\n",
    "            name for name in names[:CFG.samples_per_class]\n",
    "        ]\n",
    "\n",
    "        values_counts.append(len(paths))\n",
    "        training_samples.extend([\n",
    "            (p, num_classes) for p in paths\n",
    "        ])\n",
    "        \n",
    "        num_classes += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0afeb92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = training_samples \n",
    "value_counts = np.array(values_counts)\n",
    "CFG.n_classes = num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caac8f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(338839, 31551)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train), CFG.n_classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a24703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, hidden_size, k=3):\n",
    "        super(Head, self).__init__()\n",
    "        self.emb = nn.Linear(hidden_size, CFG.emb_size, bias=False)\n",
    "        self.dropout = utilities.Multisample_Dropout()\n",
    "        self.arc = utilities.ArcMarginProduct_subcenter(CFG.emb_size, CFG.n_classes, k)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeddings = self.dropout(x, self.emb)\n",
    "        output = self.arc(embeddings)\n",
    "        return output, F.normalize(embeddings)\n",
    "    \n",
    "class HeadV2(nn.Module):\n",
    "    def __init__(self, hidden_size, k=3):\n",
    "        super(HeadV2, self).__init__()\n",
    "        self.arc = utilities.ArcMarginProduct_subcenter(hidden_size, CFG.n_classes, k)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.arc(x)\n",
    "        return output, F.normalize(x)\n",
    "    \n",
    "class HeadV3(nn.Module):\n",
    "    def __init__(self, hidden_size, k=3):\n",
    "        super(HeadV3, self).__init__()        \n",
    "        self.emb = nn.Linear(hidden_size, CFG.emb_size, bias=False)\n",
    "        self.dropout = nn.Dropout1d(0.2)\n",
    "        self.arc = utilities.ArcMarginProduct_subcenter(CFG.emb_size, CFG.n_classes, k)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x = self.emb(x)\n",
    "        output = self.arc(x)\n",
    "        return output, F.normalize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e35b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vit_backbone, head_size, version='v1', k=3):\n",
    "        super(Model, self).__init__()\n",
    "        if version == 'v1':\n",
    "            self.head = Head(head_size, k)\n",
    "        elif version == 'v2':\n",
    "            self.head = HeadV2(head_size, k)\n",
    "        elif version == 'v3':\n",
    "            self.head = HeadV3(head_size, k)\n",
    "        else:\n",
    "            self.head = Head(head_size, k)\n",
    "        \n",
    "        self.encoder = vit_backbone.visual\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.head(x)\n",
    "\n",
    "    def get_parameters(self):\n",
    "\n",
    "        parameter_settings = [] \n",
    "        parameter_settings.extend(\n",
    "            self.get_parameter_section(\n",
    "                [(n, p) for n, p in self.encoder.named_parameters()], \n",
    "                lr=CFG.vit_bb_lr, \n",
    "                wd=CFG.vit_bb_wd\n",
    "            )\n",
    "        ) \n",
    "\n",
    "        parameter_settings.extend(\n",
    "            self.get_parameter_section(\n",
    "                [(n, p) for n, p in self.head.named_parameters()], \n",
    "                lr=CFG.hd_lr, \n",
    "                wd=CFG.hd_wd\n",
    "            )\n",
    "        ) \n",
    "\n",
    "        return parameter_settings\n",
    "\n",
    "    def get_parameter_section(self, parameters, lr=None, wd=None): \n",
    "        parameter_settings = []\n",
    "\n",
    "\n",
    "        lr_is_dict = isinstance(lr, dict)\n",
    "        wd_is_dict = isinstance(wd, dict)\n",
    "\n",
    "        layer_no = None\n",
    "        for no, (n,p) in enumerate(parameters):\n",
    "            \n",
    "            for split in n.split('.'):\n",
    "                if split.isnumeric():\n",
    "                    layer_no = int(split)\n",
    "            \n",
    "            if not layer_no:\n",
    "                layer_no = 0\n",
    "            \n",
    "            if lr_is_dict:\n",
    "                for k,v in lr.items():\n",
    "                    if layer_no < int(k):\n",
    "                        temp_lr = v\n",
    "                        break\n",
    "            else:\n",
    "                temp_lr = lr\n",
    "\n",
    "            if wd_is_dict:\n",
    "                for k,v in wd.items():\n",
    "                    if layer_no < int(k):\n",
    "                        temp_wd = v\n",
    "                        break\n",
    "            else:\n",
    "                temp_wd = wd\n",
    "\n",
    "            weight_decay = 0.0 if 'bias' in n else temp_wd\n",
    "\n",
    "            parameter_setting = {\"params\" : p, \"lr\" : temp_lr, \"weight_decay\" : temp_wd}\n",
    "\n",
    "            parameter_settings.append(parameter_setting)\n",
    "\n",
    "            #print(f'no {no} | params {n} | lr {temp_lr} | weight_decay {weight_decay} | requires_grad {p.requires_grad}')\n",
    "\n",
    "        return parameter_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5281af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ArcFace_criterion(logits_m, target, margins):\n",
    "    arc = utilities.ArcFaceLossAdaptiveMargin(margins=margins, s=CFG.s, crit=CFG.crit)\n",
    "    loss_m = arc(logits_m, target, CFG.n_classes)\n",
    "    return loss_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e7f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, scaler, scheduler, epoch):\n",
    "    model.train()\n",
    "    loss_metrics = utilities.AverageMeter()\n",
    "    criterion = ArcFace_criterion\n",
    "\n",
    "    tmp = np.sqrt(1 / np.sqrt(value_counts))\n",
    "    margins = (tmp - tmp.min()) / (tmp.max() - tmp.min()) * CFG.m + CFG.m_min\n",
    "        \n",
    "    bar = tqdm(train_loader)\n",
    "    for step, data in enumerate(bar):\n",
    "        step += 1\n",
    "        images = data['images'].to(CFG.device, dtype=torch.float)\n",
    "        labels = data['labels'].to(CFG.device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.autocast):\n",
    "            outputs, features = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels, margins)\n",
    "        loss_metrics.update(loss.item(), batch_size)\n",
    "        loss = loss / CFG.acc_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if step % CFG.acc_steps == 0 or step == len(bar):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            CFG.global_step += 1\n",
    "                        \n",
    "        lrs = utilities.get_lr_groups(optimizer.param_groups)\n",
    "\n",
    "        loss_avg = loss_metrics.avg\n",
    "\n",
    "        bar.set_postfix(loss=loss_avg, epoch=epoch, lrs=lrs, step=CFG.global_step)\n",
    "    \n",
    "@torch.no_grad()\n",
    "def val(model, valid_loader):\n",
    "    model.eval() \n",
    "\n",
    "    all_embeddings = []\n",
    "    all_labels = [] \n",
    "\n",
    "    for data in tqdm(valid_loader):\n",
    "        images = data['images'].to(CFG.device, dtype=torch.float)\n",
    "        labels = data['labels'].to(CFG.device)\n",
    "\n",
    "        _, embeddings = model(images)\n",
    "\n",
    "        all_embeddings.append(embeddings.detach().cpu().numpy())\n",
    "        all_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "    all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    return all_embeddings, all_labels\n",
    "\n",
    "def training(train_loader, \n",
    "             gallery_loader, \n",
    "             query_loader, \n",
    "             experiment_folder, \n",
    "             version='v1', \n",
    "             k=3, \n",
    "             reduce_lr_on_epoch=1,\n",
    "             use_rampup=True):\n",
    "    \n",
    "    os.makedirs(experiment_folder, exist_ok=True)\n",
    "    \n",
    "    backbone, _, _ = open_clip.create_model_and_transforms(CFG.model_name, CFG.model_data)\n",
    "\n",
    "    model = Model(backbone, CFG.hidden_layer, version, k).to(CFG.device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.get_parameters())\n",
    " \n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.autocast)\n",
    "\n",
    "    steps_per_epoch = math.ceil(len(train_loader) / CFG.acc_steps)\n",
    "\n",
    "    num_training_steps = math.ceil(CFG.n_epochs * steps_per_epoch)\n",
    "    \n",
    "    if use_rampup:\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                    num_training_steps=num_training_steps,\n",
    "                                                    num_warmup_steps=CFG.n_warmup_steps)  \n",
    "    else:\n",
    "        scheduler = get_constant_schedule(optimizer)\n",
    "        \n",
    "    best_score = 0\n",
    "    best_updated_ = 0\n",
    "    CFG.global_step = 0                   \n",
    "    for epoch in range(math.ceil(CFG.n_epochs)):\n",
    "        print(f'starting epoch {epoch}')\n",
    "\n",
    "        # train of product-10k\n",
    "        train(model, train_loader, optimizer, scaler, scheduler, epoch)\n",
    "\n",
    "        # aicrowd test data\n",
    "        print('gallery embeddings')\n",
    "        embeddings_gallery, labels_gallery = val(model, gallery_loader)\n",
    "        print('query embeddings')\n",
    "        embeddings_query, labels_query = val(model, query_loader)\n",
    "\n",
    "        # idk why it is needed\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "\n",
    "        # calculate validation score\n",
    "        _, indices = utilities.get_similiarity_l2(embeddings_gallery, embeddings_query, 1000)\n",
    "\n",
    "\n",
    "        indices = indices.tolist()\n",
    "        labels_gallery = labels_gallery.tolist()\n",
    "        labels_query = labels_query.tolist()\n",
    "\n",
    "        preds = utilities.convert_indices_to_labels(indices, labels_gallery)\n",
    "        score = utilities.map_per_set(labels_query, preds)\n",
    "        print('validation score', score)\n",
    "\n",
    "        # save model\n",
    "        torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                }, f'{experiment_folder}/model_epoch_{epoch+1}_mAP3_{score:.2f}.pt')\n",
    "\n",
    "        # save the best model\n",
    "        if score > best_score:\n",
    "            best_updated_ = 0\n",
    "            best_score = score\n",
    "\n",
    "        best_updated_ += 1\n",
    "\n",
    "        if best_updated_ >= 3:\n",
    "            print('no improvement done training....')\n",
    "            break\n",
    "            \n",
    "        if (epoch + 1) % reduce_lr_on_epoch == 0:\n",
    "            scheduler.base_lrs = [g['lr'] * CFG.reduce_lr for g in optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece494b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "\n",
    "def read_img(img_path, is_gray=False):\n",
    "    mode = cv2.IMREAD_COLOR if not is_gray else cv2.IMREAD_GRAYSCALE\n",
    "    img = cv2.imread(img_path, mode)\n",
    "    if not is_gray:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "def get_final_transform():  \n",
    "    final_transform = T.Compose([\n",
    "            T.Resize(\n",
    "                size=(CFG.image_size, CFG.image_size), \n",
    "                interpolation=T.InterpolationMode.BICUBIC,\n",
    "                antialias=True),\n",
    "            T.ToTensor(), \n",
    "            T.Normalize(\n",
    "                mean=(0.48145466, 0.4578275, 0.40821073), \n",
    "                std=(0.26862954, 0.26130258, 0.27577711)\n",
    "            )\n",
    "        ])\n",
    "    return final_transform\n",
    "\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data, \n",
    "                 transform=None, \n",
    "                 final_transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.final_transform = final_transform\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "       \n",
    "        img = read_img(self.data[idx][0])            \n",
    "        \n",
    "        if self.transform is not None:\n",
    "            if isinstance(self.transform, A.Compose):\n",
    "                img = self.transform(image=img)['image']\n",
    "            else:\n",
    "                img = self.transform(img)\n",
    "        \n",
    "        if self.final_transform is not None:\n",
    "            if isinstance(img, np.ndarray):\n",
    "                img =  Image.fromarray(img)\n",
    "            img = self.final_transform(img)\n",
    "            \n",
    "        product_id = self.data[idx][1]\n",
    "        return {\"images\": img, \"labels\": product_id}\n",
    "    \n",
    "def get_product_10k_dataloader(data_train, data_aug='image_net'):\n",
    "    \n",
    "    transform = None\n",
    "    if data_aug == 'image_net':\n",
    "        transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.AutoAugment(T.AutoAugmentPolicy.IMAGENET)\n",
    "        ])\n",
    "        \n",
    "    elif data_aug == 'aug_mix':\n",
    "        transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.AugMix()\n",
    "        ])\n",
    "    elif data_aug == 'happy_whale':\n",
    "        aug8p3 = A.OneOf([\n",
    "            A.Sharpen(p=0.3),\n",
    "            A.ToGray(p=0.3),\n",
    "            A.CLAHE(p=0.3),\n",
    "        ], p=0.5)\n",
    "\n",
    "        transform = A.Compose([\n",
    "            A.ShiftScaleRotate(rotate_limit=15, scale_limit=0.1, border_mode=cv2.BORDER_REFLECT, p=0.5),\n",
    "            A.Resize(CFG.image_size, CFG.image_size),\n",
    "            aug8p3,\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1)\n",
    "        ])\n",
    "    \n",
    "    elif data_aug == 'cut_out':        \n",
    "        transform = A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ImageCompression(quality_lower=99, quality_upper=100),\n",
    "            A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=0.7),\n",
    "            A.Resize(CFG.image_size, CFG.image_size),\n",
    "            A.Cutout(max_h_size=int(CFG.image_size * 0.4), \n",
    "                     max_w_size=int(CFG.image_size * 0.4), \n",
    "                     num_holes=1, p=0.5),\n",
    "        ])\n",
    "    elif data_aug == 'clip':\n",
    "        transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.RandomResizedCrop(\n",
    "                size=(224, 224), \n",
    "                scale=(0.9, 1.0), \n",
    "                ratio=(0.75, 1.3333), \n",
    "                interpolation=T.InterpolationMode.BICUBIC,\n",
    "                antialias=True\n",
    "            )\n",
    "        ])\n",
    "    elif data_aug == 'clip+image_net':\n",
    "        transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.AutoAugment(T.AutoAugmentPolicy.IMAGENET),\n",
    "            T.RandomResizedCrop(\n",
    "                size=(224, 224), \n",
    "                scale=(0.9, 1.0), \n",
    "                ratio=(0.75, 1.3333), \n",
    "                interpolation=T.InterpolationMode.BICUBIC,\n",
    "                antialias=True\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    final_transform = get_final_transform()\n",
    "    train_dataset = ProductDataset(data_train, \n",
    "                                   transform, \n",
    "                                   final_transform)\n",
    "    train_loader = DataLoader(train_dataset, \n",
    "                              batch_size = CFG.train_batch_size, \n",
    "                              num_workers=CFG.workers, \n",
    "                              shuffle=True, \n",
    "                              drop_last=True)\n",
    "    print(f'Training Data -> Dataset Length ({len(train_dataset)})')\n",
    "    return train_loader\n",
    "\n",
    "def aicrowd_data_loader(csv_path, img_dir='../development_test_data'):\n",
    "    df_g = pd.read_csv(csv_path)\n",
    "    df_g_ = df_g[['img_path', 'product_id']]\n",
    "    df_g_['img_path'] = df_g_.apply(lambda x: img_dir + '/' + x['img_path'], axis=1)\n",
    "    data_ = np.array(df_g_).tolist()\n",
    "    \n",
    "    final_transform = get_final_transform()\n",
    "    dataset = ProductDataset(data_, None, final_transform)\n",
    "    data_loader = DataLoader(dataset, \n",
    "                             batch_size = CFG.valid_batch_size, \n",
    "                             num_workers=CFG.workers, \n",
    "                             shuffle=False, \n",
    "                             drop_last=False)\n",
    "    \n",
    "    print(f'{csv_path} -> Dataset Length ({len(dataset)})')\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548890dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# aicrowd datasets\n",
    "gallery_loader = aicrowd_data_loader('../development_test_data/gallery.csv') \n",
    "query_loader = aicrowd_data_loader('../development_test_data/queries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4934f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3  \n",
    "version = 'v2'\n",
    "data_aug = 'happy_whale'\n",
    "CFG.reduce_lr = 0.1\n",
    "train_loader = get_product_10k_dataloader(data_train, data_aug)\n",
    "experiment_folder = f'my_experiments/{CFG.model_name}-{CFG.model_data}-{str(data_aug)}-{str(version)}-p10k-h&m-shopee-amazon-2-Arcface(k={str(k)})-All-Epoch({str(CFG.n_epochs)})-Reduce_LR_0.1'\n",
    "training(train_loader, \n",
    "         gallery_loader, \n",
    "         query_loader, \n",
    "         experiment_folder, \n",
    "         version=version,\n",
    "         k=k)\n",
    "# idk why it is needed\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c7b8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
