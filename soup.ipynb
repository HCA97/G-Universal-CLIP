{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import open_clip\n",
    "import os\n",
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResidualAttentionBlock(\n",
       "  (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "  )\n",
       "  (ls_1): Identity()\n",
       "  (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): Sequential(\n",
       "    (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "    (gelu): GELU(approximate='none')\n",
       "    (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "  )\n",
       "  (ls_2): Identity()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.resblocks[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f'my_experiments/ViT-H-14-laion2b_s32b_b79k-image_net-v2-product-10k-ArcFace(k=3)-All-Reduce-LR-1/model_epoch_6_mAP3_0.58_slim_f16.pt')\n",
    "backbone, _, _ = open_clip.create_model_and_transforms('ViT-H-14', None)\n",
    "model = backbone.visual   \n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "tmp = ()\n",
    "for m in model.transformer.resblocks:\n",
    "    tmp = tmp + ((m.ln_1, 'weight'),)\n",
    "    tmp = tmp + ((m.attn.out_proj, 'weight'),)\n",
    "    tmp = tmp + ((m.ln_2, 'weight'),)\n",
    "    tmp = tmp + ((m.mlp.c_fc, 'weight'),)\n",
    "    tmp = tmp + ((m.mlp.c_proj, 'weight'),)\n",
    "\n",
    "parameters_to_prune = tmp\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=10,\n",
    ")\n",
    "print(sum(torch.nn.utils.parameters_to_vector(model.buffers()) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, hidden_size, slim):\n",
    "        super(Head, self).__init__()\n",
    "        self.emb = nn.Linear(hidden_size, CFG.emb_size, bias=False)\n",
    "        self.slim = slim\n",
    "        if not slim:\n",
    "            self.arc = utilities.ArcMarginProduct_subcenter(CFG.emb_size, CFG.n_classes)\n",
    "\n",
    "        self.dropout = utilities.Multisample_Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.dropout(x, self.emb)\n",
    "        if not self.slim:\n",
    "            output = self.arc(embeddings)\n",
    "            return output, F.normalize(embeddings)\n",
    "        return F.normalize(embeddings)\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vit_backbone, head_size, version='v1', slim=False):\n",
    "        super(Model, self).__init__()\n",
    "        if version == 'v1': \n",
    "            self.encoder = vit_backbone.visual\n",
    "        elif version == 'v2':\n",
    "            self.encoder = vit_backbone.visual.trunk\n",
    "            \n",
    "        self.head = Head(head_size, slim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list =  [\n",
    "  f'my_experiments/ViT-H-14-laion2b_s32b_b79k-cut_out-product-10k/model_best_epoch_2_mAP3_0.55_slim.pt',\n",
    "  f'my_experiments/ViT-H-14-laion2b_s32b_b79k-happy_whale-product-10k/model_best_epoch_3_mAP3_0.54_slim.pt',\n",
    "  f'my_experiments/ViT-H-14-laion2b_s32b_b79k-None-product-10k/model_best_epoch_2_mAP3_0.53_slim.pt',\n",
    "  f'my_experiments/vit_h_224_products-10k/model_best_epoch_3_mAP3_0.53_slim.pt'\n",
    "]\n",
    "\n",
    "class CFG:\n",
    "    emb_size=512\n",
    "    model_name = 'ViT-H-14'\n",
    "    hidden_layer = 1024\n",
    "    version = 'v1'\n",
    "    n_classes=9004\n",
    "\n",
    "\n",
    "backbone, _, _ = open_clip.create_model_and_transforms(CFG.model_name, None)\n",
    "# Load models weights\n",
    "weight_list = []\n",
    "\n",
    "for path in path_list:\n",
    "    model = Model(backbone, CFG.hidden_layer, CFG.version, True)\n",
    "    model.load_state_dict(torch.load(path), strict=False)\n",
    "    weight_list.append(model.state_dict())\n",
    "\n",
    "# Average weights\n",
    "state_dict = dict((k, torch.stack([v[k] for v in weight_list]).mean(0)) for k in weight_list[0])\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "torch.save(model.state_dict(), f'my_experiments/{CFG.model_name}-soup.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slim_model(model_path, CFG):\n",
    "    name = os.path.splitext(model_path)[0]\n",
    "    \n",
    "    checkpoint = torch.load(model_path)\n",
    "    backbone, _, _ = open_clip.create_model_and_transforms(CFG.model_name, None)\n",
    "\n",
    "    model = Model(backbone, CFG.hidden_layer, CFG.version)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    model_slim = Model(backbone, CFG.hidden_layer, CFG.version, True)\n",
    "    model_slim.head.emb = model.head.emb\n",
    "    model_slim.encoder = model.encoder\n",
    "\n",
    "    torch.save(model_slim.state_dict(), \n",
    "               name + '_slim.pt')\n",
    "    \n",
    "class CFG:\n",
    "    emb_size=512\n",
    "    model_name = 'ViT-H-14'\n",
    "    hidden_layer = 1024\n",
    "    version = 'v1'\n",
    "    n_classes=9691\n",
    "    \n",
    "\n",
    "slim_model(f'my_experiments/ViT-H-14-laion2b_s32b_b79k-image_net-product-10k-all/model_epoch_1_mAP5_0.55.pt', CFG())\n",
    "slim_model(f'my_experiments/ViT-H-14-laion2b_s32b_b79k-image_net-product-10k-all/model_epoch_2_mAP5_0.55.pt', CFG())\n",
    "slim_model(f'my_experiments/ViT-H-14-laion2b_s32b_b79k-image_net-product-10k-all/model_epoch_3_mAP5_0.55.pt', CFG())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Compose(\n",
       "     RandomResizedCrop(size=(224, 224), scale=(0.9, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic), antialias=None)\n",
       "     <function _convert_to_rgb at 0x7fb1993af9d0>\n",
       "     ToTensor()\n",
       "     Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       " ),\n",
       " Compose(\n",
       "     Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)\n",
       "     CenterCrop(size=(224, 224))\n",
       "     <function _convert_to_rgb at 0x7fb1993af9d0>\n",
       "     ToTensor()\n",
       "     Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       " ))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone, t, p = open_clip.create_model_and_transforms('ViT-H-14', None)\n",
    "t, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = torch.load(f'my_experiments/ViT-H-14-laion2b_s32b_b79k-image_net-v2-product-10k/model_epoch_2_mAP3_0.57.pt')\n",
    "backbone, _, _ = open_clip.create_model_and_transforms('ViT-H-14', None)\n",
    "\n",
    "model = Model(backbone, 1024, 'v2')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model_slim = Model(backbone, 1024, 'v2', True)\n",
    "model_slim.head.emb = model.head.emb\n",
    "model_slim.encoder = model.encoder\n",
    "\n",
    "\n",
    "torch.save(model_slim.state_dict(), \n",
    "           f'my_experiments/ViT-L-14-laion2b_s32b_b82k-image_net-v2-product-10k-ArcFace(k=3)-All/model_best_epoch_4_mAP3_0.55_slim.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import open_clip\n",
    "import os\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Head, self).__init__()\n",
    "        self.arc = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = x\n",
    "        output = self.arc(embeddings)\n",
    "        return output, F.normalize(embeddings)\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vit_backbone, head_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = vit_backbone.visual   \n",
    "        self.head = Head(head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.head(x)\n",
    "    \n",
    "checkpoint = torch.load(f'/home/cemmi/Documents/aicrowd/G-Universal-CLIP/my_experiments/ViT-H-14-laion2b_s32b_b79k-happy_whale-v2-product-10k-ArcFace(k=3)-All-Epoch(10)-Reduce_LR_0.1/model_epoch_2_mAP3_0.57.pt')\n",
    "backbone, _, _ = open_clip.create_model_and_transforms('ViT-H-14', None)\n",
    "model = Model(backbone, 1024)\n",
    "model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "\n",
    "model_slim = model.encoder\n",
    "model_slim.half()\n",
    "\n",
    "torch.save(model_slim.state_dict(), \n",
    "           f'/home/cemmi/Documents/aicrowd/G-Universal-CLIP/my_experiments/ViT-H-14-laion2b_s32b_b79k-happy_whale-v2-product-10k-ArcFace(k=3)-All-Epoch(10)-Reduce_LR_0.1/model_epoch_2_mAP3_0.57_slim.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mHead\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_size):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(Head, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Head, self).__init__()\n",
    "        self.arc = utilities.ArcMarginProduct_subcenter(768, 9691, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = x\n",
    "        output = self.arc(embeddings)\n",
    "        return output, F.normalize(embeddings)\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vit_backbone, head_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = vit_backbone.visual   \n",
    "        self.head = Head(head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.head(x)\n",
    "    \n",
    "checkpoint = torch.load(f'/home/cemmi/Documents/aicrowd/G-Universal-CLIP/my_experiments/ViT-H-14-laion2b_s32b_b79k-happy_whale-v2-product-10k-ArcFace(k=3)-All-Epoch(10)-Reduce_LR_0.1/model_epoch_2_mAP3_0.57.pt')\n",
    "backbone, _, _ = open_clip.create_model_and_transforms('ViT-L-14', None)\n",
    "model = Model(backbone, 1024)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model_slim = model.encoder\n",
    "\n",
    "\n",
    "torch.save(model_slim.state_dict(), \n",
    "           f'/home/cemmi/Documents/aicrowd/G-Universal-CLIP/my_experiments/ViT-H-14-laion2b_s32b_b79k-happy_whale-v2-product-10k-ArcFace(k=3)-All-Epoch(10)-Reduce_LR_0.1/model_epoch_2_mAP3_0.57_slim.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f'my_experiments/ViT-H-14-laion2b_s32b_b79k-image_net-v2-product-10k/model_best_epoch_2_mAP3_0.57_slim.pt')\n",
    "backbone, _, _ = open_clip.create_model_and_transforms('ViT-H-14', None)\n",
    "model = backbone.visual\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import utilities\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Head, self).__init__()\n",
    "        self.arc = utilities.ArcMarginProduct_subcenter(1024, 9004, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = x\n",
    "        output = self.arc(embeddings)\n",
    "        return output, F.normalize(embeddings)\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vit_backbone, head_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = vit_backbone.visual   \n",
    "        self.head = Head(head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.head(x)\n",
    "    \n",
    "\n",
    "checkpoint = torch.load(f'my_experiments/ViT-H-14-laion2b_s32b_b79k-image_net-v2-product-10k/model_epoch_2_mAP3_0.57.pt')\n",
    "backbone, _, _ = open_clip.create_model_and_transforms('ViT-H-14', None)\n",
    "model = Model(backbone, 1024)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model_slim = model.encoder\n",
    "model_slim.half()\n",
    "\n",
    "frozen_mod = torch.jit.optimize_for_inference(torch.jit.script(model_slim.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_mod.save(f'my_experiments/ViT-H-14-laion2b_s32b_b79k-image_net-v2-product-10k/model_epoch_2_mAP3_0.57_slim_f16_jit.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = th.rand((10000, 512)).cuda()\n",
    "b = th.rand((9000, 512)).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "\n",
    "c = th.cdist(b, a)\n",
    "i = th.argsort(c, dim=1)[:, :1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3824, device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0].argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3824, device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict_product_ranks(self):\n",
    "        \"\"\"\n",
    "        This function should return a numpy array of shape `(num_queries, 1000)`. \n",
    "        For ach query image your model will need to predict \n",
    "        a set of 1000 unique gallery indexes, in order of best match first.\n",
    "\n",
    "        Outputs:\n",
    "            class_ranks - A 2D numpy array where the axes correspond to:\n",
    "                          axis 0 - Batch size\n",
    "                          axis 1 - An ordered rank list of matched image indexes, most confident prediction first\n",
    "                            - maximum length of this should be 1000\n",
    "                            - predictions above this limit will be dropped\n",
    "                            - duplicates will be dropped such that the lowest index entry is preserved\n",
    "        \"\"\"\n",
    "\n",
    "        gallery_dataset = SubmissionDataset(\n",
    "            root=self.dataset_path, annotation_file=self.gallery_csv_path,\n",
    "            transforms=self.final_transform\n",
    "        )\n",
    "\n",
    "        gallery_loader = DataLoader(\n",
    "            gallery_dataset, batch_size=self.batch_size,\n",
    "            shuffle=False, pin_memory=True, num_workers=self.num_workers\n",
    "        )\n",
    "\n",
    "        query_dataset = SubmissionDataset(\n",
    "            root=self.dataset_path, annotation_file=self.queries_csv_path,\n",
    "            transforms=self.final_transform, with_bbox=True\n",
    "        )\n",
    "\n",
    "        query_loader = DataLoader(\n",
    "            query_dataset, batch_size=self.batch_size,\n",
    "            shuffle=False, pin_memory=True, num_workers=self.num_workers\n",
    "        )\n",
    "\n",
    "        print('Calculating embeddings')\n",
    "        gallery_embeddings = th.zeros((len(gallery_dataset), self.embedding_shape), dtype=th.float16, device=self.device)\n",
    "        query_embeddings = th.zeros((len(query_dataset), self.embedding_shape), dtype=th.float16, device=self.device)\n",
    "        s = time.time()\n",
    "        with th.no_grad():\n",
    "            for i, images in tqdm(enumerate(gallery_loader),\n",
    "                                total=len(gallery_loader)):\n",
    "                images = images.to(self.device)\n",
    "                outputs = self.model(images.half())\n",
    "                outputs = th.squeeze(outputs.data)\n",
    "                gallery_embeddings[\n",
    "                    i*self.batch_size:(i*self.batch_size + self.batch_size), :\n",
    "                ] = outputs\n",
    "            \n",
    "            for i, images in tqdm(enumerate(query_loader),\n",
    "                                total=len(query_loader)):\n",
    "                images = images.to(self.device)\n",
    "                outputs = self.model(images.half())\n",
    "                outputs = th.squeeze(outputs.data)\n",
    "                query_embeddings[\n",
    "                    i*self.batch_size:(i*self.batch_size + self.batch_size), :\n",
    "                ] = outputs\n",
    "        e = time.time()\n",
    "        print(f'Total Time Passed to Calculate the Embeddings {e - s}s')\n",
    "\n",
    "        print('Calculating distances')\n",
    "        s = time.time()        \n",
    "        dist = th.cdist(query_embeddings, gallery_embeddings)\n",
    "        ind = th.argsort(dist, dim=1)[:, :1000]\n",
    "        ind = ind.data.cpu().numpy()\n",
    "        \n",
    "        e = time.time()\n",
    "        print(f'Total Time Passed to Calculate Distance {e - s}s')\n",
    "        return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.48 ðŸš€ Python-3.8.16 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3090, 24252MiB)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unsupported type encountered! See docs for supported types https://docs.ultralytics.com/predict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov8n.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m image \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mrand((\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m320\u001b[39m,\u001b[38;5;241m640\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/ultralytics/yolo/engine/model.py:99\u001b[0m, in \u001b[0;36mYOLO.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/ultralytics/yolo/engine/model.py:220\u001b[0m, in \u001b[0;36mYOLO.predict\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m get_cfg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs, overrides)\n\u001b[1;32m    219\u001b[0m is_cli \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolo\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124multralytics\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/ultralytics/yolo/engine/predictor.py:114\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/torch/autograd/grad_mode.py:43\u001b[0m, in \u001b[0;36m_DecoratorContextManager._wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 43\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/ultralytics/yolo/engine/predictor.py:146\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_model(model)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# setup source every time predict is called\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# check if save_dir/ label file exists\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_txt:\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/ultralytics/yolo/engine/predictor.py:128\u001b[0m, in \u001b[0;36mBasePredictor.setup_source\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# predict, segment\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     transforms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_inference_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mvid_stride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvid_stride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mauto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39msource_type\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvid_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvid_writer \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mbs, [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mbs\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/ultralytics/yolo/data/build.py:164\u001b[0m, in \u001b[0;36mload_inference_source\u001b[0;34m(source, transforms, imgsz, vid_stride, stride, auto)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03mTODO: docs\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# source\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m source, webcam, screenshot, from_img, in_memory \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m source_type \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39msource_type \u001b[38;5;28;01mif\u001b[39;00m in_memory \u001b[38;5;28;01melse\u001b[39;00m SourceTypes(webcam, screenshot, from_img)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Dataloader\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/ultralytics/yolo/data/build.py:153\u001b[0m, in \u001b[0;36mcheck_source\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m    151\u001b[0m     from_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported type encountered! See docs for supported types https://docs.ultralytics.com/predict\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m source, webcam, screenshot, from_img, in_memory\n",
      "\u001b[0;31mException\u001b[0m: Unsupported type encountered! See docs for supported types https://docs.ultralytics.com/predict"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch as th\n",
    "\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "image = th.rand((16,3,320,640))\n",
    "results = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch as th\n",
    "\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\").model.to('cuda')\n",
    "image = th.rand((16,3,320,640), device='cuda')\n",
    "results = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 84, 4200])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = results\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 144, 40, 80]),\n",
       " torch.Size([16, 144, 20, 40]),\n",
       " torch.Size([16, 144, 10, 20]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1, b2, b3 = b\n",
    "b1.shape, b2.shape, b3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.0.48'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ultralytics.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4ee870ab444af8a8689fba9fdb6a16993f9af4d6f8c51486b98fd7ee4129479"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
