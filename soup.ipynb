{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "import torch_pruning as tp\n",
    "\n",
    "model = resnet18(pretrained=True).eval()\n",
    "\n",
    "# 1. build dependency graph for resnet18\n",
    "DG = tp.DependencyGraph().build_dependency(model, example_inputs=torch.randn(1,3,224,224))\n",
    "\n",
    "# 2. Specify the to-be-pruned channels. Here we prune those channels indexed by [2, 6, 9].\n",
    "group = DG.get_pruning_group( model.conv1, tp.prune_conv_out_channels, idxs=[2, 6, 9] )\n",
    "\n",
    "# 3. prune all grouped layers that are coupled with model.conv1 (included).\n",
    "if DG.check_pruning_group(group): # avoid full pruning, i.e., channels=0.\n",
    "    group.prune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 61, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on fc (Linear(in_features=512, out_features=997, bias=True)) => prune_out_channels on fc (Linear(in_features=512, out_features=997, bias=True)), #idxs=1000\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on layer4.0.downsample.0 (Conv2d(256, 509, kernel_size=(1, 1), stride=(2, 2), bias=False)) => prune_out_channels on layer4.0.downsample.0 (Conv2d(256, 509, kernel_size=(1, 1), stride=(2, 2), bias=False)), #idxs=512\n",
      "[1] prune_out_channels on layer4.0.downsample.0 (Conv2d(256, 509, kernel_size=(1, 1), stride=(2, 2), bias=False)) => prune_out_channels on layer4.0.downsample.1 (BatchNorm2d(509, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=512\n",
      "[2] prune_out_channels on layer4.0.downsample.1 (BatchNorm2d(509, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_6(AddBackward0), #idxs=512\n",
      "[3] prune_out_channels on _ElementWiseOp_6(AddBackward0) => prune_out_channels on layer4.0.bn2 (BatchNorm2d(509, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=512\n",
      "[4] prune_out_channels on _ElementWiseOp_6(AddBackward0) => prune_out_channels on _ElementWiseOp_5(ReluBackward0), #idxs=512\n",
      "[5] prune_out_channels on _ElementWiseOp_5(ReluBackward0) => prune_out_channels on _ElementWiseOp_4(AddBackward0), #idxs=512\n",
      "[6] prune_out_channels on _ElementWiseOp_5(ReluBackward0) => prune_in_channels on layer4.1.conv1 (Conv2d(509, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), #idxs=512\n",
      "[7] prune_out_channels on _ElementWiseOp_4(AddBackward0) => prune_out_channels on layer4.1.bn2 (BatchNorm2d(509, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=512\n",
      "[8] prune_out_channels on _ElementWiseOp_4(AddBackward0) => prune_out_channels on _ElementWiseOp_3(ReluBackward0), #idxs=512\n",
      "[9] prune_out_channels on _ElementWiseOp_3(ReluBackward0) => prune_out_channels on _ElementWiseOp_2(MeanBackward1), #idxs=512\n",
      "[10] prune_out_channels on _ElementWiseOp_2(MeanBackward1) => prune_out_channels on _Reshape_0(), #idxs=512\n",
      "[11] prune_out_channels on _Reshape_0() => prune_in_channels on fc (Linear(in_features=509, out_features=997, bias=True)), #idxs=512\n",
      "[12] prune_in_channels on fc (Linear(in_features=509, out_features=997, bias=True)) => prune_out_channels on _ElementWiseOp_1(TBackward0), #idxs=512\n",
      "[13] prune_out_channels on layer4.1.bn2 (BatchNorm2d(509, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on layer4.1.conv2 (Conv2d(512, 509, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), #idxs=512\n",
      "[14] prune_out_channels on layer4.0.bn2 (BatchNorm2d(509, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on layer4.0.conv2 (Conv2d(512, 509, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), #idxs=512\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on layer3.0.downsample.0 (Conv2d(128, 253, kernel_size=(1, 1), stride=(2, 2), bias=False)) => prune_out_channels on layer3.0.downsample.0 (Conv2d(128, 253, kernel_size=(1, 1), stride=(2, 2), bias=False)), #idxs=256\n",
      "[1] prune_out_channels on layer3.0.downsample.0 (Conv2d(128, 253, kernel_size=(1, 1), stride=(2, 2), bias=False)) => prune_out_channels on layer3.0.downsample.1 (BatchNorm2d(253, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=256\n",
      "[2] prune_out_channels on layer3.0.downsample.1 (BatchNorm2d(253, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_10(AddBackward0), #idxs=256\n",
      "[3] prune_out_channels on _ElementWiseOp_10(AddBackward0) => prune_out_channels on layer3.0.bn2 (BatchNorm2d(253, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=256\n",
      "[4] prune_out_channels on _ElementWiseOp_10(AddBackward0) => prune_out_channels on _ElementWiseOp_9(ReluBackward0), #idxs=256\n",
      "[5] prune_out_channels on _ElementWiseOp_9(ReluBackward0) => prune_out_channels on _ElementWiseOp_8(AddBackward0), #idxs=256\n",
      "[6] prune_out_channels on _ElementWiseOp_9(ReluBackward0) => prune_in_channels on layer3.1.conv1 (Conv2d(253, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), #idxs=256\n",
      "[7] prune_out_channels on _ElementWiseOp_8(AddBackward0) => prune_out_channels on layer3.1.bn2 (BatchNorm2d(253, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=256\n",
      "[8] prune_out_channels on _ElementWiseOp_8(AddBackward0) => prune_out_channels on _ElementWiseOp_7(ReluBackward0), #idxs=256\n",
      "[9] prune_out_channels on _ElementWiseOp_7(ReluBackward0) => prune_in_channels on layer4.0.downsample.0 (Conv2d(253, 509, kernel_size=(1, 1), stride=(2, 2), bias=False)), #idxs=256\n",
      "[10] prune_out_channels on _ElementWiseOp_7(ReluBackward0) => prune_in_channels on layer4.0.conv1 (Conv2d(253, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)), #idxs=256\n",
      "[11] prune_out_channels on layer3.1.bn2 (BatchNorm2d(253, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on layer3.1.conv2 (Conv2d(256, 253, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), #idxs=256\n",
      "[12] prune_out_channels on layer3.0.bn2 (BatchNorm2d(253, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on layer3.0.conv2 (Conv2d(256, 253, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), #idxs=256\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on layer2.0.downsample.0 (Conv2d(61, 125, kernel_size=(1, 1), stride=(2, 2), bias=False)) => prune_out_channels on layer2.0.downsample.0 (Conv2d(61, 125, kernel_size=(1, 1), stride=(2, 2), bias=False)), #idxs=128\n",
      "[1] prune_out_channels on layer2.0.downsample.0 (Conv2d(61, 125, kernel_size=(1, 1), stride=(2, 2), bias=False)) => prune_out_channels on layer2.0.downsample.1 (BatchNorm2d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=128\n",
      "[2] prune_out_channels on layer2.0.downsample.1 (BatchNorm2d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_14(AddBackward0), #idxs=128\n",
      "[3] prune_out_channels on _ElementWiseOp_14(AddBackward0) => prune_out_channels on layer2.0.bn2 (BatchNorm2d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=128\n",
      "[4] prune_out_channels on _ElementWiseOp_14(AddBackward0) => prune_out_channels on _ElementWiseOp_13(ReluBackward0), #idxs=128\n",
      "[5] prune_out_channels on _ElementWiseOp_13(ReluBackward0) => prune_out_channels on _ElementWiseOp_12(AddBackward0), #idxs=128\n",
      "[6] prune_out_channels on _ElementWiseOp_13(ReluBackward0) => prune_in_channels on layer2.1.conv1 (Conv2d(125, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), #idxs=128\n",
      "[7] prune_out_channels on _ElementWiseOp_12(AddBackward0) => prune_out_channels on layer2.1.bn2 (BatchNorm2d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=128\n",
      "[8] prune_out_channels on _ElementWiseOp_12(AddBackward0) => prune_out_channels on _ElementWiseOp_11(ReluBackward0), #idxs=128\n",
      "[9] prune_out_channels on _ElementWiseOp_11(ReluBackward0) => prune_in_channels on layer3.0.downsample.0 (Conv2d(125, 253, kernel_size=(1, 1), stride=(2, 2), bias=False)), #idxs=128\n",
      "[10] prune_out_channels on _ElementWiseOp_11(ReluBackward0) => prune_in_channels on layer3.0.conv1 (Conv2d(125, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)), #idxs=128\n",
      "[11] prune_out_channels on layer2.1.bn2 (BatchNorm2d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on layer2.1.conv2 (Conv2d(128, 125, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), #idxs=128\n",
      "[12] prune_out_channels on layer2.0.bn2 (BatchNorm2d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on layer2.0.conv2 (Conv2d(128, 125, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), #idxs=128\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on layer1.0.conv1 (Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)) => prune_out_channels on layer1.0.conv1 (Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), #idxs=64\n",
      "[1] prune_out_channels on layer1.0.conv1 (Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)) => prune_out_channels on layer1.0.bn1 (BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=64\n",
      "[2] prune_out_channels on layer1.0.bn1 (BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_21(ReluBackward0), #idxs=64\n",
      "[3] prune_out_channels on _ElementWiseOp_21(ReluBackward0) => prune_in_channels on layer1.0.conv2 (Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), #idxs=64\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on layer1.1.conv1 (Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)) => prune_out_channels on layer1.1.conv1 (Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), #idxs=64\n",
      "[1] prune_out_channels on layer1.1.conv1 (Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)) => prune_out_channels on layer1.1.bn1 (BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=64\n",
      "[2] prune_out_channels on layer1.1.bn1 (BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_22(ReluBackward0), #idxs=64\n",
      "[3] prune_out_channels on _ElementWiseOp_22(ReluBackward0) => prune_in_channels on layer1.1.conv2 (Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), #idxs=64\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on layer2.0.conv1 (Conv2d(61, 125, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)) => prune_out_channels on layer2.0.conv1 (Conv2d(61, 125, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)), #idxs=128\n",
      "[1] prune_out_channels on layer2.0.conv1 (Conv2d(61, 125, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)) => prune_out_channels on layer2.0.bn1 (BatchNorm2d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=128\n",
      "[2] prune_out_channels on layer2.0.bn1 (BatchNorm2d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_23(ReluBackward0), #idxs=128\n",
      "[3] prune_out_channels on _ElementWiseOp_23(ReluBackward0) => prune_in_channels on layer2.0.conv2 (Conv2d(125, 125, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), #idxs=128\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on layer2.1.conv1 (Conv2d(125, 125, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)) => prune_out_channels on layer2.1.conv1 (Conv2d(125, 125, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), #idxs=128\n",
      "[1] prune_out_channels on layer2.1.conv1 (Conv2d(125, 125, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)) => prune_out_channels on layer2.1.bn1 (BatchNorm2d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=128\n",
      "[2] prune_out_channels on layer2.1.bn1 (BatchNorm2d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_24(ReluBackward0), #idxs=128\n",
      "[3] prune_out_channels on _ElementWiseOp_24(ReluBackward0) => prune_in_channels on layer2.1.conv2 (Conv2d(125, 125, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), #idxs=128\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on layer3.0.conv1 (Conv2d(125, 253, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)) => prune_out_channels on layer3.0.conv1 (Conv2d(125, 253, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)), #idxs=256\n",
      "[1] prune_out_channels on layer3.0.conv1 (Conv2d(125, 253, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)) => prune_out_channels on layer3.0.bn1 (BatchNorm2d(253, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=256\n",
      "[2] prune_out_channels on layer3.0.bn1 (BatchNorm2d(253, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_25(ReluBackward0), #idxs=256\n",
      "[3] prune_out_channels on _ElementWiseOp_25(ReluBackward0) => prune_in_channels on layer3.0.conv2 (Conv2d(253, 253, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), #idxs=256\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on layer3.1.conv1 (Conv2d(253, 253, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)) => prune_out_channels on layer3.1.conv1 (Conv2d(253, 253, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), #idxs=256\n",
      "[1] prune_out_channels on layer3.1.conv1 (Conv2d(253, 253, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)) => prune_out_channels on layer3.1.bn1 (BatchNorm2d(253, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=256\n",
      "[2] prune_out_channels on layer3.1.bn1 (BatchNorm2d(253, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_26(ReluBackward0), #idxs=256\n",
      "[3] prune_out_channels on _ElementWiseOp_26(ReluBackward0) => prune_in_channels on layer3.1.conv2 (Conv2d(253, 253, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), #idxs=256\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on layer4.0.conv1 (Conv2d(253, 509, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)) => prune_out_channels on layer4.0.conv1 (Conv2d(253, 509, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)), #idxs=512\n",
      "[1] prune_out_channels on layer4.0.conv1 (Conv2d(253, 509, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)) => prune_out_channels on layer4.0.bn1 (BatchNorm2d(509, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=512\n",
      "[2] prune_out_channels on layer4.0.bn1 (BatchNorm2d(509, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_27(ReluBackward0), #idxs=512\n",
      "[3] prune_out_channels on _ElementWiseOp_27(ReluBackward0) => prune_in_channels on layer4.0.conv2 (Conv2d(509, 509, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), #idxs=512\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "          Pruning Group\n",
      "--------------------------------\n",
      "[0] prune_out_channels on layer4.1.conv1 (Conv2d(509, 509, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)) => prune_out_channels on layer4.1.conv1 (Conv2d(509, 509, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), #idxs=512\n",
      "[1] prune_out_channels on layer4.1.conv1 (Conv2d(509, 509, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)) => prune_out_channels on layer4.1.bn1 (BatchNorm2d(509, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), #idxs=512\n",
      "[2] prune_out_channels on layer4.1.bn1 (BatchNorm2d(509, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_28(ReluBackward0), #idxs=512\n",
      "[3] prune_out_channels on _ElementWiseOp_28(ReluBackward0) => prune_in_channels on layer4.1.conv2 (Conv2d(509, 509, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), #idxs=512\n",
      "--------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "for group in DG.get_all_groups(ignored_layers=[model.conv1], root_module_types=[nn.Conv2d, nn.Linear]):\n",
    "    # handle groups in sequential order\n",
    "    idxs = [2,4,6] # your pruning indices\n",
    "    group.prune(idxs=idxs)\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv1): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "  (patch_dropout): Identity()\n",
       "  (ln_pre): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (12): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (13): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (14): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (15): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (16): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (17): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (18): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (19): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (20): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (21): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (22): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (23): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (24): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (25): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (26): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (27): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (28): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (29): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (30): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (31): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "\n",
    "# Load the trained model\n",
    "checkpoint = torch.load(f'my_experiments/soup_slim_p10k_h_m_image_net_happy_whale_w_03.pt')\n",
    "backbone, _, _ = open_clip.create_model_and_transforms('ViT-H-14', None)\n",
    "model = backbone.visual   \n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(1,3,224,224).cuda()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1280, 1024])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "├─Conv2d: 1-1                                      [-1, 1280, 16, 16]        752,640\n",
      "├─Identity: 1-2                                    [-1, 257, 1280]           --\n",
      "├─LayerNorm: 1-3                                   [-1, 257, 1280]           2,560\n",
      "├─Transformer: 1-4                                 [-1, 2, 1280]             --\n",
      "|    └─ModuleList: 2                               []                        --\n",
      "|    |    └─ResidualAttentionBlock: 3-1            [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-2            [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-3            [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-4            [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-5            [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-6            [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-7            [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-8            [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-9            [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-10           [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-11           [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-12           [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-13           [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-14           [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-15           [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-16           [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-17           [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-18           [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-19           [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-20           [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-21           [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-22           [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-23           [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-24           [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-25           [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-26           [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-27           [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-28           [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-29           [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-30           [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-31           [-1, 2, 1280]             19,677,440\n",
      "|    |    └─ResidualAttentionBlock: 3-32           [-1, 2, 1280]             19,677,440\n",
      "├─LayerNorm: 1-5                                   [-1, 1280]                2,560\n",
      "====================================================================================================\n",
      "Total params: 630,435,840\n",
      "Trainable params: 630,435,840\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 2.08\n",
      "====================================================================================================\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 6.27\n",
      "Params size (MB): 2404.92\n",
      "Estimated Total Size (MB): 2411.77\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "├─Conv2d: 1-1                                      [-1, 1280, 16, 16]        752,640\n",
       "├─Identity: 1-2                                    [-1, 257, 1280]           --\n",
       "├─LayerNorm: 1-3                                   [-1, 257, 1280]           2,560\n",
       "├─Transformer: 1-4                                 [-1, 2, 1280]             --\n",
       "|    └─ModuleList: 2                               []                        --\n",
       "|    |    └─ResidualAttentionBlock: 3-1            [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-2            [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-3            [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-4            [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-5            [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-6            [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-7            [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-8            [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-9            [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-10           [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-11           [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-12           [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-13           [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-14           [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-15           [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-16           [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-17           [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-18           [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-19           [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-20           [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-21           [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-22           [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-23           [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-24           [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-25           [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-26           [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-27           [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-28           [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-29           [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-30           [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-31           [-1, 2, 1280]             19,677,440\n",
       "|    |    └─ResidualAttentionBlock: 3-32           [-1, 2, 1280]             19,677,440\n",
       "├─LayerNorm: 1-5                                   [-1, 1280]                2,560\n",
       "====================================================================================================\n",
       "Total params: 630,435,840\n",
       "Trainable params: 630,435,840\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 2.08\n",
       "====================================================================================================\n",
       "Input size (MB): 0.57\n",
       "Forward/backward pass size (MB): 6.27\n",
       "Params size (MB): 2404.92\n",
       "Estimated Total Size (MB): 2411.77\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from torchsummary import summary\n",
    "model.cuda()\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get product 10k\n",
    "import os\n",
    "import math\n",
    "\n",
    "\n",
    "import numpy as np\n",
    " \n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import open_clip\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import utilities\n",
    "\n",
    "def read_img(img_path, is_gray=False):\n",
    "    mode = cv2.IMREAD_COLOR if not is_gray else cv2.IMREAD_GRAYSCALE\n",
    "    img = cv2.imread(img_path, mode)\n",
    "    if not is_gray:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 img_dir, \n",
    "                 annotations_file, \n",
    "                 transform=None, \n",
    "                 final_transform=None, \n",
    "                 headers=None,\n",
    "                 test_mode=False):\n",
    "        self.data = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.final_transform = final_transform\n",
    "        self.headers = {\"img_path\": \"img_path\", \"product_id\": \"product_id\"}\n",
    "        if headers:\n",
    "            self.headers = headers\n",
    "        self.test_mode = test_mode\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.data[self.headers[\"img_path\"]][idx])\n",
    "        \n",
    "        img = read_img(img_path)\n",
    "        if self.test_mode:\n",
    "            x, y, w, h = self.data[\"bbox_x\"][idx], self.data[\"bbox_y\"][idx], \\\n",
    "                         self.data[\"bbox_w\"][idx], self.data[\"bbox_h\"][idx]\n",
    "            img = img[y:y+h, x:x+w]\n",
    "            \n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = transform(image=img)[\"image\"]\n",
    "        \n",
    "        if self.final_transform is not None:\n",
    "            if isinstance(img, np.ndarray):\n",
    "                img =  Image.fromarray(img)\n",
    "            img = self.final_transform(img)\n",
    "            \n",
    "        product_id = self.data[self.headers[\"product_id\"]][idx]\n",
    "        return img, product_id\n",
    "    \n",
    "def get_final_transform():  \n",
    "    final_transform = T.Compose([\n",
    "            T.Resize(\n",
    "                size=(224, 224), \n",
    "                interpolation=T.InterpolationMode.BICUBIC,\n",
    "                antialias=True),\n",
    "            T.ToTensor(), \n",
    "            T.Normalize(\n",
    "                mean=(0.48145466, 0.4578275, 0.40821073), \n",
    "                std=(0.26862954, 0.26130258, 0.27577711)\n",
    "            )\n",
    "        ])\n",
    "    return final_transform\n",
    "\n",
    "final_transform = get_final_transform()\n",
    "img_dir = \"../development_test_data\"\n",
    "dataset_test = ProductDataset(img_dir, os.path.join(img_dir, \"queries.csv\"), None, final_transform, test_mode=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=32, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cemmi/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "  0%|                                                                                                                           | 0/61 [00:51<?, ?it/s]\n",
      "/home/cemmi/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/torch/ao/quantization/observer.py:1204: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# Specify quantization configuration\n",
    "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Prepare the model for static quantization\n",
    "torch.quantization.prepare(model, inplace=True)\n",
    "\n",
    "# Calibrate the prepared model to determine quantization parameters\n",
    "with torch.no_grad():\n",
    "    for imgs, p_id in tqdm(dataloader_test):\n",
    "        model(imgs)\n",
    "        break\n",
    "\n",
    "# Convert to a quantized model\n",
    "quantized_model = torch.quantization.convert(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'quantized::conv2d.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d.new' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nQuantizedCPU: registered at ../aten/src/ATen/native/quantized/cpu/qconv.cpp:1449 [kernel]\nQuantizedCUDA: registered at ../aten/src/ATen/native/quantized/cudnn/Conv.cpp:418 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:35 [backend fallback]\nAutogradCPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:39 [backend fallback]\nAutogradCUDA: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:47 [backend fallback]\nAutogradXLA: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:51 [backend fallback]\nAutogradMPS: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:59 [backend fallback]\nAutogradXPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:43 [backend fallback]\nAutogradHPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:68 [backend fallback]\nAutogradLazy: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:55 [backend fallback]\nTracer: registered at ../torch/csrc/autograd/TraceTypeManual.cpp:296 [backend fallback]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m quantized_model\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      2\u001b[0m imgs\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mquantized_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/open_clip/transformer.py:447\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 447\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape = [*, width, grid, grid]\u001b[39;00m\n\u001b[1;32m    448\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape = [*, width, grid ** 2]\u001b[39;00m\n\u001b[1;32m    449\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape = [*, grid ** 2, width]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/torch/ao/nn/quantized/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    460\u001b[0m     _reversed_padding_repeated_twice \u001b[38;5;241m=\u001b[39m _reverse_repeat_padding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding)\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, _reversed_padding_repeated_twice,\n\u001b[1;32m    462\u001b[0m                   mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode)\n\u001b[0;32m--> 463\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_packed_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_point\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/torch/_ops.py:442\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'quantized::conv2d.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d.new' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nQuantizedCPU: registered at ../aten/src/ATen/native/quantized/cpu/qconv.cpp:1449 [kernel]\nQuantizedCUDA: registered at ../aten/src/ATen/native/quantized/cudnn/Conv.cpp:418 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:35 [backend fallback]\nAutogradCPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:39 [backend fallback]\nAutogradCUDA: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:47 [backend fallback]\nAutogradXLA: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:51 [backend fallback]\nAutogradMPS: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:59 [backend fallback]\nAutogradXPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:43 [backend fallback]\nAutogradHPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:68 [backend fallback]\nAutogradLazy: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:55 [backend fallback]\nTracer: registered at ../torch/csrc/autograd/TraceTypeManual.cpp:296 [backend fallback]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "quantized_model.cuda()\n",
    "imgs.cuda()\n",
    "quantized_model(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "import torch\n",
    "import open_clip\n",
    "\n",
    "# Load your pre-trained ViT-H model\n",
    "checkpoint = torch.load(f'my_experiments/soup_slim_p10k_h_m_image_net_happy_whale_w_03.pt')\n",
    "backbone, _, _ = open_clip.create_model_and_transforms('ViT-H-14', None)\n",
    "model = backbone.visual   \n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "model.half()\n",
    "\n",
    "# Set the device to run on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Create some dummy input data in the shape of your model's input\n",
    "dummy_input = torch.randn(1, 3, 224, 224, device=device).half()\n",
    "\n",
    "# Define the dynamic axes for the input and output tensors\n",
    "input_names = [\"input\"]\n",
    "output_names = [\"output\"]\n",
    "dynamic_axes = {\n",
    "    \"input\": {0: \"batch_size\"},\n",
    "    \"output\": {0: \"batch_size\"}\n",
    "}\n",
    "\n",
    "# Export the model to ONNX format with dynamic axes\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"my_experiments/vit-h.onnx\",\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    dynamic_axes=dynamic_axes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-13 22:52:39.919030688 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:541 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/reference/execution-providers/CUDA-ExecutionProvider.html#requirements to ensure all dependencies are met.\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as rt\n",
    "import numpy as np\n",
    "\n",
    "# Load the ONNX model\n",
    "sess = rt.InferenceSession(\"my_experiments/vit-h-quantize.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
    "\n",
    "input_data = np.random.random((10, 3, 224, 224)).astype(np.float16)\n",
    "\n",
    "output = sess.run(None, {\"input\": input_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.0.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.1.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.2.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.3.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.4.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.5.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.6.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.7.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.8.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.9.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.10.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.11.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.12.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.13.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.14.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.15.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.16.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.17.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.18.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.19.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.20.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.21.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.22.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.23.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.24.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.25.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.26.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.27.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.28.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.29.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.30.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: InsertedCast_transformer.resblocks.31.attn.out_proj.weight_Transposed. Please add data type info for this tensor if your model has customized operators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.0/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.0/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.0/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.0/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.0/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.0/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.1/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.1/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.1/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.1/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.1/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.1/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.2/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.2/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.2/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.2/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.2/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.2/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.3/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.3/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.3/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.3/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.3/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.3/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.4/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.4/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.4/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.4/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.4/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.4/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.5/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.5/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.5/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.5/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.5/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.5/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.6/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.6/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.6/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.6/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.6/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.6/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.7/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.7/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.7/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.7/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.7/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.7/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.8/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.8/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.8/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.8/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.8/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.8/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.9/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.9/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.9/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.9/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.9/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.9/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.10/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.10/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.10/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.10/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.10/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.10/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.11/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.11/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.11/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.11/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.11/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.11/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.12/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.12/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.12/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.12/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.12/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.12/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.13/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.13/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.13/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.13/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.13/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.13/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.14/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.14/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.14/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.14/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.14/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.14/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.15/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.15/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.15/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.15/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.15/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.15/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.16/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.16/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.16/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.16/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.16/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.16/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.17/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.17/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.17/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.17/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.17/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.17/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.18/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.18/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.18/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.18/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.18/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.18/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.19/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.19/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.19/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.19/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.19/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.19/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.20/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.20/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.20/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.20/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.20/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.20/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.21/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.21/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.21/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.21/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.21/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.21/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.22/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.22/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.22/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.22/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.22/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.22/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.23/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.23/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.23/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.23/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.23/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.23/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.24/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.24/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.24/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.24/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.24/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.24/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.25/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.25/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.25/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.25/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.25/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.25/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.26/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.26/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.26/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.26/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.26/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.26/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.27/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.27/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.27/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.27/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.27/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.27/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.28/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.28/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.28/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.28/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.28/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.28/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.29/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.29/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.29/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.29/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.29/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.29/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.30/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.30/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.30/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.30/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.30/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.30/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.31/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.31/attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.31/attn/MatMul_2]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.31/attn/Gemm_MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.31/mlp/c_fc/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/resblocks.31/mlp/c_proj/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/MatMul]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "quantized_model = quantize_dynamic(\"my_experiments/vit-h.onnx\", \"my_experiments/vit-h-quantize.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prunning conv1\n",
      "prunning transformer.resblocks.0.attn.out_proj\n",
      "prunning transformer.resblocks.0.mlp.c_fc\n",
      "prunning transformer.resblocks.0.mlp.c_proj\n",
      "prunning transformer.resblocks.1.attn.out_proj\n",
      "prunning transformer.resblocks.1.mlp.c_fc\n",
      "prunning transformer.resblocks.1.mlp.c_proj\n",
      "prunning transformer.resblocks.2.attn.out_proj\n",
      "prunning transformer.resblocks.2.mlp.c_fc\n",
      "prunning transformer.resblocks.2.mlp.c_proj\n",
      "prunning transformer.resblocks.3.attn.out_proj\n",
      "prunning transformer.resblocks.3.mlp.c_fc\n",
      "prunning transformer.resblocks.3.mlp.c_proj\n",
      "prunning transformer.resblocks.4.attn.out_proj\n",
      "prunning transformer.resblocks.4.mlp.c_fc\n",
      "prunning transformer.resblocks.4.mlp.c_proj\n",
      "prunning transformer.resblocks.5.attn.out_proj\n",
      "prunning transformer.resblocks.5.mlp.c_fc\n",
      "prunning transformer.resblocks.5.mlp.c_proj\n",
      "prunning transformer.resblocks.6.attn.out_proj\n",
      "prunning transformer.resblocks.6.mlp.c_fc\n",
      "prunning transformer.resblocks.6.mlp.c_proj\n",
      "prunning transformer.resblocks.7.attn.out_proj\n",
      "prunning transformer.resblocks.7.mlp.c_fc\n",
      "prunning transformer.resblocks.7.mlp.c_proj\n",
      "prunning transformer.resblocks.8.attn.out_proj\n",
      "prunning transformer.resblocks.8.mlp.c_fc\n",
      "prunning transformer.resblocks.8.mlp.c_proj\n",
      "prunning transformer.resblocks.9.attn.out_proj\n",
      "prunning transformer.resblocks.9.mlp.c_fc\n",
      "prunning transformer.resblocks.9.mlp.c_proj\n",
      "prunning transformer.resblocks.10.attn.out_proj\n",
      "prunning transformer.resblocks.10.mlp.c_fc\n",
      "prunning transformer.resblocks.10.mlp.c_proj\n",
      "prunning transformer.resblocks.11.attn.out_proj\n",
      "prunning transformer.resblocks.11.mlp.c_fc\n",
      "prunning transformer.resblocks.11.mlp.c_proj\n",
      "prunning transformer.resblocks.12.attn.out_proj\n",
      "prunning transformer.resblocks.12.mlp.c_fc\n",
      "prunning transformer.resblocks.12.mlp.c_proj\n",
      "prunning transformer.resblocks.13.attn.out_proj\n",
      "prunning transformer.resblocks.13.mlp.c_fc\n",
      "prunning transformer.resblocks.13.mlp.c_proj\n",
      "prunning transformer.resblocks.14.attn.out_proj\n",
      "prunning transformer.resblocks.14.mlp.c_fc\n",
      "prunning transformer.resblocks.14.mlp.c_proj\n",
      "prunning transformer.resblocks.15.attn.out_proj\n",
      "prunning transformer.resblocks.15.mlp.c_fc\n",
      "prunning transformer.resblocks.15.mlp.c_proj\n",
      "prunning transformer.resblocks.16.attn.out_proj\n",
      "prunning transformer.resblocks.16.mlp.c_fc\n",
      "prunning transformer.resblocks.16.mlp.c_proj\n",
      "prunning transformer.resblocks.17.attn.out_proj\n",
      "prunning transformer.resblocks.17.mlp.c_fc\n",
      "prunning transformer.resblocks.17.mlp.c_proj\n",
      "prunning transformer.resblocks.18.attn.out_proj\n",
      "prunning transformer.resblocks.18.mlp.c_fc\n",
      "prunning transformer.resblocks.18.mlp.c_proj\n",
      "prunning transformer.resblocks.19.attn.out_proj\n",
      "prunning transformer.resblocks.19.mlp.c_fc\n",
      "prunning transformer.resblocks.19.mlp.c_proj\n",
      "prunning transformer.resblocks.20.attn.out_proj\n",
      "prunning transformer.resblocks.20.mlp.c_fc\n",
      "prunning transformer.resblocks.20.mlp.c_proj\n",
      "prunning transformer.resblocks.21.attn.out_proj\n",
      "prunning transformer.resblocks.21.mlp.c_fc\n",
      "prunning transformer.resblocks.21.mlp.c_proj\n",
      "prunning transformer.resblocks.22.attn.out_proj\n",
      "prunning transformer.resblocks.22.mlp.c_fc\n",
      "prunning transformer.resblocks.22.mlp.c_proj\n",
      "prunning transformer.resblocks.23.attn.out_proj\n",
      "prunning transformer.resblocks.23.mlp.c_fc\n",
      "prunning transformer.resblocks.23.mlp.c_proj\n",
      "prunning transformer.resblocks.24.attn.out_proj\n",
      "prunning transformer.resblocks.24.mlp.c_fc\n",
      "prunning transformer.resblocks.24.mlp.c_proj\n",
      "prunning transformer.resblocks.25.attn.out_proj\n",
      "prunning transformer.resblocks.25.mlp.c_fc\n",
      "prunning transformer.resblocks.25.mlp.c_proj\n",
      "prunning transformer.resblocks.26.attn.out_proj\n",
      "prunning transformer.resblocks.26.mlp.c_fc\n",
      "prunning transformer.resblocks.26.mlp.c_proj\n",
      "prunning transformer.resblocks.27.attn.out_proj\n",
      "prunning transformer.resblocks.27.mlp.c_fc\n",
      "prunning transformer.resblocks.27.mlp.c_proj\n",
      "prunning transformer.resblocks.28.attn.out_proj\n",
      "prunning transformer.resblocks.28.mlp.c_fc\n",
      "prunning transformer.resblocks.28.mlp.c_proj\n",
      "prunning transformer.resblocks.29.attn.out_proj\n",
      "prunning transformer.resblocks.29.mlp.c_fc\n",
      "prunning transformer.resblocks.29.mlp.c_proj\n",
      "prunning transformer.resblocks.30.attn.out_proj\n",
      "prunning transformer.resblocks.30.mlp.c_fc\n",
      "prunning transformer.resblocks.30.mlp.c_proj\n",
      "prunning transformer.resblocks.31.attn.out_proj\n",
      "prunning transformer.resblocks.31.mlp.c_fc\n",
      "prunning transformer.resblocks.31.mlp.c_proj\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "import torch.nn.utils.prune as prune\n",
    "import tqdm\n",
    "\n",
    "def prune_model_v1(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            print(f'prunning {name}')\n",
    "            prune.l1_unstructured(module, name='weight', amount=0.1)\n",
    "            prune.remove(module, 'weight')\n",
    "        elif isinstance(module, torch.nn.Linear):\n",
    "            print(f'prunning {name}')\n",
    "            prune.l1_unstructured(module, name='weight', amount=0.1)\n",
    "            prune.remove(module, 'weight')\n",
    "            \n",
    "    return model\n",
    "\n",
    "def prune_model_v2(model):\n",
    "    parameters_to_prune = ()\n",
    "    for m in model.transformer.resblocks:\n",
    "        parameters_to_prune = parameters_to_prune + ((m.ln_1, 'weight'),)\n",
    "        parameters_to_prune = parameters_to_prune + ((m.attn.out_proj, 'weight'),)\n",
    "        parameters_to_prune = parameters_to_prune + ((m.ln_2, 'weight'),)\n",
    "    \n",
    "    prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=0.1,\n",
    "    )\n",
    "    \n",
    "    for module, _ in parameters_to_prune:\n",
    "        prune.remove(module, 'weight')\n",
    "        \n",
    "    return model\n",
    "    \n",
    "checkpoint = torch.load(f'my_experiments/soup_slim_p10k_h_m_image_net_happy_whale_w_03.pt')\n",
    "backbone, _, _ = open_clip.create_model_and_transforms('ViT-H-14', None)\n",
    "model = backbone.visual   \n",
    "model.load_state_dict(checkpoint)\n",
    "model = prune_model_v1(model)\n",
    "model.half()\n",
    "torch.save(model.state_dict(), f'my_experiments/prunning_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, hidden_size, slim):\n",
    "        super(Head, self).__init__()\n",
    "        self.emb = nn.Linear(hidden_size, CFG.emb_size, bias=False)\n",
    "        self.slim = slim\n",
    "        if not slim:\n",
    "            self.arc = utilities.ArcMarginProduct_subcenter(CFG.emb_size, CFG.n_classes)\n",
    "\n",
    "        self.dropout = utilities.Multisample_Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.dropout(x, self.emb)\n",
    "        if not self.slim:\n",
    "            output = self.arc(embeddings)\n",
    "            return output, F.normalize(embeddings)\n",
    "        return F.normalize(embeddings)\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vit_backbone, head_size, version='v1', slim=False):\n",
    "        super(Model, self).__init__()\n",
    "        if version == 'v1': \n",
    "            self.encoder = vit_backbone.visual\n",
    "        elif version == 'v2':\n",
    "            self.encoder = vit_backbone.visual.trunk\n",
    "            \n",
    "        self.head = Head(head_size, slim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list =  [\n",
    "  f'my_experiments/ViT-H-14-laion2b_s32b_b79k-cut_out-product-10k/model_best_epoch_2_mAP3_0.55_slim.pt',\n",
    "  f'my_experiments/ViT-H-14-laion2b_s32b_b79k-happy_whale-product-10k/model_best_epoch_3_mAP3_0.54_slim.pt',\n",
    "  f'my_experiments/ViT-H-14-laion2b_s32b_b79k-None-product-10k/model_best_epoch_2_mAP3_0.53_slim.pt',\n",
    "  f'my_experiments/vit_h_224_products-10k/model_best_epoch_3_mAP3_0.53_slim.pt'\n",
    "]\n",
    "\n",
    "class CFG:\n",
    "    emb_size=512\n",
    "    model_name = 'ViT-H-14'\n",
    "    hidden_layer = 1024\n",
    "    version = 'v1'\n",
    "    n_classes=9004\n",
    "\n",
    "\n",
    "backbone, _, _ = open_clip.create_model_and_transforms(CFG.model_name, None)\n",
    "# Load models weights\n",
    "weight_list = []\n",
    "\n",
    "for path in path_list:\n",
    "    model = Model(backbone, CFG.hidden_layer, CFG.version, True)\n",
    "    model.load_state_dict(torch.load(path), strict=False)\n",
    "    weight_list.append(model.state_dict())\n",
    "\n",
    "# Average weights\n",
    "state_dict = dict((k, torch.stack([v[k] for v in weight_list]).mean(0)) for k in weight_list[0])\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "torch.save(model.state_dict(), f'my_experiments/{CFG.model_name}-soup.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slim_model(model_path, CFG):\n",
    "    name = os.path.splitext(model_path)[0]\n",
    "    \n",
    "    checkpoint = torch.load(model_path)\n",
    "    backbone, _, _ = open_clip.create_model_and_transforms(CFG.model_name, None)\n",
    "\n",
    "    model = Model(backbone, CFG.hidden_layer, CFG.version)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    model_slim = Model(backbone, CFG.hidden_layer, CFG.version, True)\n",
    "    model_slim.head.emb = model.head.emb\n",
    "    model_slim.encoder = model.encoder\n",
    "\n",
    "    torch.save(model_slim.state_dict(), \n",
    "               name + '_slim.pt')\n",
    "    \n",
    "class CFG:\n",
    "    emb_size=512\n",
    "    model_name = 'ViT-H-14'\n",
    "    hidden_layer = 1024\n",
    "    version = 'v1'\n",
    "    n_classes=9691\n",
    "    \n",
    "\n",
    "slim_model(f'my_experiments/ViT-H-14-laion2b_s32b_b79k-image_net-product-10k-all/model_epoch_1_mAP5_0.55.pt', CFG())\n",
    "slim_model(f'my_experiments/ViT-H-14-laion2b_s32b_b79k-image_net-product-10k-all/model_epoch_2_mAP5_0.55.pt', CFG())\n",
    "slim_model(f'my_experiments/ViT-H-14-laion2b_s32b_b79k-image_net-product-10k-all/model_epoch_3_mAP5_0.55.pt', CFG())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Compose(\n",
       "     RandomResizedCrop(size=(224, 224), scale=(0.9, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic), antialias=None)\n",
       "     <function _convert_to_rgb at 0x7fb1993af9d0>\n",
       "     ToTensor()\n",
       "     Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       " ),\n",
       " Compose(\n",
       "     Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)\n",
       "     CenterCrop(size=(224, 224))\n",
       "     <function _convert_to_rgb at 0x7fb1993af9d0>\n",
       "     ToTensor()\n",
       "     Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       " ))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone, t, p = open_clip.create_model_and_transforms('ViT-H-14', None)\n",
    "t, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = torch.load(f'my_experiments/ViT-H-14-laion2b_s32b_b79k-image_net-v2-product-10k/model_epoch_2_mAP3_0.57.pt')\n",
    "backbone, _, _ = open_clip.create_model_and_transforms('ViT-H-14', None)\n",
    "\n",
    "model = Model(backbone, 1024, 'v2')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model_slim = Model(backbone, 1024, 'v2', True)\n",
    "model_slim.head.emb = model.head.emb\n",
    "model_slim.encoder = model.encoder\n",
    "\n",
    "\n",
    "torch.save(model_slim.state_dict(), \n",
    "           f'my_experiments/ViT-L-14-laion2b_s32b_b82k-image_net-v2-product-10k-ArcFace(k=3)-All/model_best_epoch_4_mAP3_0.55_slim.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import open_clip\n",
    "import os\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Head, self).__init__()\n",
    "        self.arc = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = x\n",
    "        output = self.arc(embeddings)\n",
    "        return output, F.normalize(embeddings)\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vit_backbone, head_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = vit_backbone.visual   \n",
    "        self.head = Head(head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.head(x)\n",
    "    \n",
    "checkpoint = torch.load(f'/home/cemmi/Documents/aicrowd/G-Universal-CLIP/my_experiments/ViT-H-14-laion2b_s32b_b79k-happy_whale-v2-p10k-h&m-amazon-2-Arcface(k=3)-All-Epoch(10)-Reduce_LR_0.1/model_epoch_2_mAP3_0.55.pt')\n",
    "backbone, _, _ = open_clip.create_model_and_transforms('ViT-H-14', None)\n",
    "model = Model(backbone, 1024)\n",
    "model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "\n",
    "model_slim = model.encoder\n",
    "model_slim.half()\n",
    "\n",
    "torch.save(model_slim.state_dict(), \n",
    "           f'/home/cemmi/Documents/aicrowd/G-Universal-CLIP/my_experiments/ViT-H-14-laion2b_s32b_b79k-happy_whale-v2-p10k-h&m-amazon-2-Arcface(k=3)-All-Epoch(10)-Reduce_LR_0.1/model_epoch_2_mAP3_0.55_slim.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mHead\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_size):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(Head, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Head, self).__init__()\n",
    "        self.arc = utilities.ArcMarginProduct_subcenter(768, 9691, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = x\n",
    "        output = self.arc(embeddings)\n",
    "        return output, F.normalize(embeddings)\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vit_backbone, head_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = vit_backbone.visual   \n",
    "        self.head = Head(head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.head(x)\n",
    "    \n",
    "checkpoint = torch.load(f'/home/cemmi/Documents/aicrowd/G-Universal-CLIP/my_experiments/ViT-H-14-laion2b_s32b_b79k-happy_whale-v2-product-10k-ArcFace(k=3)-All-Epoch(10)-Reduce_LR_0.1/model_epoch_2_mAP3_0.57.pt')\n",
    "backbone, _, _ = open_clip.create_model_and_transforms('ViT-L-14', None)\n",
    "model = Model(backbone, 1024)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model_slim = model.encoder\n",
    "\n",
    "\n",
    "torch.save(model_slim.state_dict(), \n",
    "           f'/home/cemmi/Documents/aicrowd/G-Universal-CLIP/my_experiments/ViT-H-14-laion2b_s32b_b79k-happy_whale-v2-product-10k-ArcFace(k=3)-All-Epoch(10)-Reduce_LR_0.1/model_epoch_2_mAP3_0.57_slim.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f'my_experiments/ViT-H-14-laion2b_s32b_b79k-image_net-v2-product-10k/model_best_epoch_2_mAP3_0.57_slim.pt')\n",
    "backbone, _, _ = open_clip.create_model_and_transforms('ViT-H-14', None)\n",
    "model = backbone.visual\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import utilities\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Head, self).__init__()\n",
    "        self.arc = utilities.ArcMarginProduct_subcenter(1024, 9004, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = x\n",
    "        output = self.arc(embeddings)\n",
    "        return output, F.normalize(embeddings)\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vit_backbone, head_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = vit_backbone.visual   \n",
    "        self.head = Head(head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.head(x)\n",
    "    \n",
    "\n",
    "checkpoint = torch.load(f'my_experiments/ViT-H-14-laion2b_s32b_b79k-image_net-v2-product-10k/model_epoch_2_mAP3_0.57.pt')\n",
    "backbone, _, _ = open_clip.create_model_and_transforms('ViT-H-14', None)\n",
    "model = Model(backbone, 1024)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model_slim = model.encoder\n",
    "model_slim.half()\n",
    "\n",
    "frozen_mod = torch.jit.optimize_for_inference(torch.jit.script(model_slim.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_mod.save(f'my_experiments/ViT-H-14-laion2b_s32b_b79k-image_net-v2-product-10k/model_epoch_2_mAP3_0.57_slim_f16_jit.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = th.rand((10000, 512)).cuda()\n",
    "b = th.rand((9000, 512)).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "\n",
    "c = th.cdist(b, a)\n",
    "i = th.argsort(c, dim=1)[:, :1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3824, device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0].argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3824, device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict_product_ranks(self):\n",
    "        \"\"\"\n",
    "        This function should return a numpy array of shape `(num_queries, 1000)`. \n",
    "        For ach query image your model will need to predict \n",
    "        a set of 1000 unique gallery indexes, in order of best match first.\n",
    "\n",
    "        Outputs:\n",
    "            class_ranks - A 2D numpy array where the axes correspond to:\n",
    "                          axis 0 - Batch size\n",
    "                          axis 1 - An ordered rank list of matched image indexes, most confident prediction first\n",
    "                            - maximum length of this should be 1000\n",
    "                            - predictions above this limit will be dropped\n",
    "                            - duplicates will be dropped such that the lowest index entry is preserved\n",
    "        \"\"\"\n",
    "\n",
    "        gallery_dataset = SubmissionDataset(\n",
    "            root=self.dataset_path, annotation_file=self.gallery_csv_path,\n",
    "            transforms=self.final_transform\n",
    "        )\n",
    "\n",
    "        gallery_loader = DataLoader(\n",
    "            gallery_dataset, batch_size=self.batch_size,\n",
    "            shuffle=False, pin_memory=True, num_workers=self.num_workers\n",
    "        )\n",
    "\n",
    "        query_dataset = SubmissionDataset(\n",
    "            root=self.dataset_path, annotation_file=self.queries_csv_path,\n",
    "            transforms=self.final_transform, with_bbox=True\n",
    "        )\n",
    "\n",
    "        query_loader = DataLoader(\n",
    "            query_dataset, batch_size=self.batch_size,\n",
    "            shuffle=False, pin_memory=True, num_workers=self.num_workers\n",
    "        )\n",
    "\n",
    "        print('Calculating embeddings')\n",
    "        gallery_embeddings = th.zeros((len(gallery_dataset), self.embedding_shape), dtype=th.float16, device=self.device)\n",
    "        query_embeddings = th.zeros((len(query_dataset), self.embedding_shape), dtype=th.float16, device=self.device)\n",
    "        s = time.time()\n",
    "        with th.no_grad():\n",
    "            for i, images in tqdm(enumerate(gallery_loader),\n",
    "                                total=len(gallery_loader)):\n",
    "                images = images.to(self.device)\n",
    "                outputs = self.model(images.half())\n",
    "                outputs = th.squeeze(outputs.data)\n",
    "                gallery_embeddings[\n",
    "                    i*self.batch_size:(i*self.batch_size + self.batch_size), :\n",
    "                ] = outputs\n",
    "            \n",
    "            for i, images in tqdm(enumerate(query_loader),\n",
    "                                total=len(query_loader)):\n",
    "                images = images.to(self.device)\n",
    "                outputs = self.model(images.half())\n",
    "                outputs = th.squeeze(outputs.data)\n",
    "                query_embeddings[\n",
    "                    i*self.batch_size:(i*self.batch_size + self.batch_size), :\n",
    "                ] = outputs\n",
    "        e = time.time()\n",
    "        print(f'Total Time Passed to Calculate the Embeddings {e - s}s')\n",
    "\n",
    "        print('Calculating distances')\n",
    "        s = time.time()        \n",
    "        dist = th.cdist(query_embeddings, gallery_embeddings)\n",
    "        ind = th.argsort(dist, dim=1)[:, :1000]\n",
    "        ind = ind.data.cpu().numpy()\n",
    "        \n",
    "        e = time.time()\n",
    "        print(f'Total Time Passed to Calculate Distance {e - s}s')\n",
    "        return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.48 🚀 Python-3.8.16 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3090, 24252MiB)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unsupported type encountered! See docs for supported types https://docs.ultralytics.com/predict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov8n.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m image \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mrand((\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m320\u001b[39m,\u001b[38;5;241m640\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/ultralytics/yolo/engine/model.py:99\u001b[0m, in \u001b[0;36mYOLO.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/ultralytics/yolo/engine/model.py:220\u001b[0m, in \u001b[0;36mYOLO.predict\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m get_cfg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs, overrides)\n\u001b[1;32m    219\u001b[0m is_cli \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolo\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124multralytics\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/ultralytics/yolo/engine/predictor.py:114\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/torch/autograd/grad_mode.py:43\u001b[0m, in \u001b[0;36m_DecoratorContextManager._wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 43\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/ultralytics/yolo/engine/predictor.py:146\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_model(model)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# setup source every time predict is called\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# check if save_dir/ label file exists\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_txt:\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/ultralytics/yolo/engine/predictor.py:128\u001b[0m, in \u001b[0;36mBasePredictor.setup_source\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# predict, segment\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     transforms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_inference_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mvid_stride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvid_stride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mauto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39msource_type\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvid_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvid_writer \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mbs, [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mbs\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/ultralytics/yolo/data/build.py:164\u001b[0m, in \u001b[0;36mload_inference_source\u001b[0;34m(source, transforms, imgsz, vid_stride, stride, auto)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03mTODO: docs\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# source\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m source, webcam, screenshot, from_img, in_memory \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m source_type \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39msource_type \u001b[38;5;28;01mif\u001b[39;00m in_memory \u001b[38;5;28;01melse\u001b[39;00m SourceTypes(webcam, screenshot, from_img)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Dataloader\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/ultralytics/yolo/data/build.py:153\u001b[0m, in \u001b[0;36mcheck_source\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m    151\u001b[0m     from_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported type encountered! See docs for supported types https://docs.ultralytics.com/predict\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m source, webcam, screenshot, from_img, in_memory\n",
      "\u001b[0;31mException\u001b[0m: Unsupported type encountered! See docs for supported types https://docs.ultralytics.com/predict"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch as th\n",
    "\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "image = th.rand((16,3,320,640))\n",
    "results = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch as th\n",
    "\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\").model.to('cuda')\n",
    "image = th.rand((16,3,320,640), device='cuda')\n",
    "results = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 84, 4200])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = results\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 144, 40, 80]),\n",
       " torch.Size([16, 144, 20, 40]),\n",
       " torch.Size([16, 144, 10, 20]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1, b2, b3 = b\n",
    "b1.shape, b2.shape, b3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.0.48'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ultralytics.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4ee870ab444af8a8689fba9fdb6a16993f9af4d6f8c51486b98fd7ee4129479"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
