{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import utilities\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import timm\n",
    "import math\n",
    "from transformers import (get_linear_schedule_with_warmup, \n",
    "                          get_cosine_schedule_with_warmup, \n",
    "                          get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "                          get_constant_schedule_with_warmup)\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import random\n",
    "import gc\n",
    "import transformers\n",
    "from transformers import CLIPProcessor, CLIPVisionModel,  CLIPVisionConfig\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from pytorch_metric_learning import losses\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name = 'ViT-L-14-336' \n",
    "    model_data = 'openai'\n",
    "    samples_per_class = 10000000\n",
    "    min_samples = 2\n",
    "    image_size = 336 \n",
    "    seed = 5\n",
    "    workers = 6\n",
    "    train_batch_size = 8\n",
    "    valid_batch_size = 32 \n",
    "    emb_size = 512\n",
    "    vit_bb_lr = {'8': 1.25e-6, '16': 2.5e-6, '20': 5e-6, '24': 10e-6} \n",
    "    vit_bb_wd = 1e-3\n",
    "    hd_lr = 3e-4\n",
    "    hd_wd = 1e-5\n",
    "    autocast = True\n",
    "    n_warmup_steps = 1000\n",
    "    n_epochs = 1\n",
    "    device = torch.device('cuda')\n",
    "    s=30.\n",
    "    m=.45\n",
    "    m_min=.05\n",
    "    freeze_norm = False\n",
    "    acc_steps = 4\n",
    "    global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_backbone, model_transforms, _ = open_clip.create_model_and_transforms(CFG.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Head, self).__init__()\n",
    "\n",
    "        self.emb = nn.Linear(hidden_size, CFG.emb_size, bias=False)\n",
    "        self.arc = None\n",
    "        self.dropout = utilities.Multisample_Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.dropout(x, self.emb)\n",
    "        \n",
    "        output = self.arc(embeddings)\n",
    "\n",
    "        return output, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vit_backbone):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.vit_backbone = vit_backbone\n",
    "\n",
    "        self.head = Head(768)\n",
    "\n",
    "    def forward(self, images):\n",
    "\n",
    "        x = transforms.functional.resize(images, size=[CFG.image_size, CFG.image_size]) \n",
    "        x = x/255\n",
    "        x = transforms.functional.normalize(x,  \n",
    "                                             mean=model_transforms.transforms[-1].mean, \n",
    "                                             std=model_transforms.transforms[-1].std)\n",
    "\n",
    "        x = self.vit_backbone.encode_image(x)\n",
    "        \n",
    "        return self.head(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "path_list =  [\n",
    "              '/home/ivan/Desktop/GUIE/models/soup-v1/ViT-L-14-336',\n",
    "              '/home/ivan/Desktop/GUIE/models/soup-v2/ViT-L-14-336',\n",
    "              '/home/ivan/Desktop/GUIE/models/soup-v3/ViT-L-14-336',\n",
    "              '/home/ivan/Desktop/GUIE/models/soup-v4/ViT-L-14-336'\n",
    "              ]\n",
    "\n",
    "# Load models weights\n",
    "weight_list = []\n",
    "\n",
    "model = Model(vit_backbone)\n",
    "weight_list.append(model.state_dict())\n",
    "\n",
    "for path in path_list:\n",
    "    model = Model(vit_backbone)\n",
    "    model.load_state_dict(torch.load(path), strict=False)\n",
    "    weight_list.append(model.state_dict())\n",
    "\n",
    "# Average weights\n",
    "state_dict = dict((k, torch.stack([v[k] for v in weight_list]).mean(0)) for k in weight_list[0])\n",
    "model.load_state_dict(state_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = CFG.model_name.replace('/','-')\n",
    "torch.save(model.state_dict(), f'../models/{model_name}-soup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4ee870ab444af8a8689fba9fdb6a16993f9af4d6f8c51486b98fd7ee4129479"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
