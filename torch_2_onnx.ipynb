{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a77fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import utilities\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import timm\n",
    "import math\n",
    "from transformers import (get_linear_schedule_with_warmup, \n",
    "                          get_cosine_schedule_with_warmup, \n",
    "                          get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "                          get_constant_schedule_with_warmup)\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import random\n",
    "import gc\n",
    "import transformers\n",
    "from transformers import CLIPProcessor, CLIPVisionModel,  CLIPVisionConfig\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from pytorch_metric_learning import losses\n",
    "import open_clip\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import torch.onnx\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1282bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head_Slim(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Head_Slim, self).__init__()\n",
    "\n",
    "        self.emb = nn.Linear(hidden_size, 512, bias=False)\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout((i+1)*.1) for i in range(5)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x_1 = self.emb(self.dropouts[0](x))\n",
    "        x_2 = self.emb(self.dropouts[1](x))\n",
    "        x_3 = self.emb(self.dropouts[2](x))\n",
    "        x_4 = self.emb(self.dropouts[3](x))\n",
    "        x_5 = self.emb(self.dropouts[4](x))\n",
    "\n",
    "        return F.normalize((x_1 + x_2 + x_3 + x_4 + x_5) / 5.0)\n",
    "\n",
    "class Model_Slim(nn.Module):\n",
    "    def __init__(self, vit_backbone):\n",
    "        super(Model_Slim, self).__init__()\n",
    "        self.encoder = vit_backbone.visual\n",
    "        self.head = Head_Slim(1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1594caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_backbone, _, _ = open_clip.create_model_and_transforms('ViT-H-14', None)\n",
    "model = Model_Slim(vit_backbone)\n",
    "model.load_state_dict(torch.load(\"./my_experiments/vit_h_224_products-10k/model_best_epoch_0_mAP3_0.53_slim.pt\"))\n",
    "model.eval()\n",
    "model.half()\n",
    "\n",
    "model_ = torch.jit.script(model)\n",
    "# model.to('cuda')\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48130db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ = model_.to('cuda')\n",
    "next(model_.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e79ca98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 3, 224, 224, dtype=torch.float16)\n",
    "dummy_input = dummy_input.to('cuda')\n",
    "dummy_input.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e9996",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model_, \n",
    "    dummy_input, \n",
    "    \"./my_experiments/dmy/model_best_epoch_0_mAP3_0.53_slim.onnx\", \n",
    "    opset_version = 16,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb9933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"./my_experiments/dmy/model_best_epoch_0_mAP3_0.53_slim.onnx\")\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "print(onnx.helper.printable_graph(model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebec52b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d3ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_tensorrt\n",
    "\n",
    "inputs = [\n",
    "    torch_tensorrt.Input(\n",
    "        min_shape=[1, 3, 224, 224],\n",
    "        opt_shape=[1, 3, 224, 224],\n",
    "        max_shape=[1, 3, 224, 224],\n",
    "        dtype=torch.half,\n",
    "    )\n",
    "]\n",
    "enabled_precisions = {torch.float, torch.half}  # Run with fp16\n",
    "\n",
    "trt_ts_module = torch_tensorrt.compile(\n",
    "    model, inputs=inputs, enabled_precisions=enabled_precisions\n",
    ")\n",
    "\n",
    "input_data = torch.randn(1, 3, 224, 224)\n",
    "input_data = input_data.to(\"cuda\").half()\n",
    "result = trt_ts_module(input_data)\n",
    "torch.jit.save(trt_ts_module, \"trt_ts_module.ts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f4cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import utilities\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import timm\n",
    "import math\n",
    "from transformers import (get_linear_schedule_with_warmup, \n",
    "                          get_cosine_schedule_with_warmup, \n",
    "                          get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "                          get_constant_schedule_with_warmup)\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import random\n",
    "import gc\n",
    "import transformers\n",
    "from transformers import CLIPProcessor, CLIPVisionModel,  CLIPVisionConfig\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from pytorch_metric_learning import losses\n",
    "import open_clip\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import torch.onnx\n",
    "\n",
    "class Head_Slim(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Head_Slim, self).__init__()\n",
    "\n",
    "        self.emb = nn.Linear(hidden_size, 512, bias=False)\n",
    "        self.dropout = utilities.Multisample_Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.dropout(x, self.emb)\n",
    "\n",
    "        return F.normalize(embeddings)\n",
    "\n",
    "class Model_Slim(nn.Module):\n",
    "    def __init__(self, vit_backbone):\n",
    "        super(Model_Slim, self).__init__()\n",
    "        self.encoder = vit_backbone.visual\n",
    "        self.head = Head_Slim(1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.head(x)\n",
    "\n",
    "vit_backbone, _, _ = open_clip.create_model_and_transforms('ViT-H-14', None)\n",
    "model = Model_Slim(vit_backbone)\n",
    "model.load_state_dict(torch.load(\"./my_experiments/vit_h_224_products-10k/model_best_epoch_0_mAP3_0.53_slim.pt\"))\n",
    "model.eval()\n",
    "model.cuda()\n",
    "# model.half()\n",
    "\n",
    "input = torch.randn(1, 3, 224, 224, requires_grad=True, device='cuda')\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    input, \n",
    "    \"./my_experiments/dmy/model_best_epoch_0_mAP3_0.53_slim.onnx\", \n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    verbose=True,\n",
    "    export_params=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicrowd-product",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "58995ac93f621c7301292278c5e7bacfcfe4a6e6e3b571fc0ea7acaa2147e276"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
