{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a77fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import utilities\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import timm\n",
    "import math\n",
    "from transformers import (get_linear_schedule_with_warmup, \n",
    "                          get_cosine_schedule_with_warmup, \n",
    "                          get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "                          get_constant_schedule_with_warmup)\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import random\n",
    "import gc\n",
    "import transformers\n",
    "from transformers import CLIPProcessor, CLIPVisionModel,  CLIPVisionConfig\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from pytorch_metric_learning import losses\n",
    "import open_clip\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import torch.onnx\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1282bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head_Slim(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Head_Slim, self).__init__()\n",
    "\n",
    "        self.emb = nn.Linear(hidden_size, 512, bias=False)\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout((i+1)*.1) for i in range(5)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x_1 = self.emb(self.dropouts[0](x))\n",
    "        x_2 = self.emb(self.dropouts[1](x))\n",
    "        x_3 = self.emb(self.dropouts[2](x))\n",
    "        x_4 = self.emb(self.dropouts[3](x))\n",
    "        x_5 = self.emb(self.dropouts[4](x))\n",
    "\n",
    "        return F.normalize((x_1 + x_2 + x_3 + x_4 + x_5) / 5.0)\n",
    "\n",
    "class Model_Slim(nn.Module):\n",
    "    def __init__(self, vit_backbone):\n",
    "        super(Model_Slim, self).__init__()\n",
    "        self.encoder = vit_backbone.visual\n",
    "        self.head = Head_Slim(1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1594caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_backbone, _, _ = open_clip.create_model_and_transforms('ViT-H-14', None)\n",
    "model = Model_Slim(vit_backbone)\n",
    "model.load_state_dict(torch.load(\"./my_experiments/vit_h_224_products-10k/model_best_epoch_0_mAP3_0.53_slim.pt\"))\n",
    "model.eval()\n",
    "model.half()\n",
    "\n",
    "model_ = torch.jit.script(model)\n",
    "# model.to('cuda')\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48130db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ = model_.to('cuda')\n",
    "next(model_.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e79ca98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 3, 224, 224, dtype=torch.float16)\n",
    "dummy_input = dummy_input.to('cuda')\n",
    "dummy_input.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e9996",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model_, \n",
    "    dummy_input, \n",
    "    \"./my_experiments/dmy/model_best_epoch_0_mAP3_0.53_slim_.onnx\", \n",
    "    opset_version = 15,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb9933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"./my_experiments/dmy/model_best_epoch_0_mAP3_0.53_slim.onnx\")\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "print(onnx.helper.printable_graph(model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1d3ba7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loaded ViT-H-14 model config.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "outputs_[i]->uses().empty() INTERNAL ASSERT FAILED at \"../torch/csrc/jit/ir/ir.cpp\":1309, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 59\u001b[0m\n\u001b[1;32m     49\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     50\u001b[0m     torch_tensorrt\u001b[38;5;241m.\u001b[39mInput(\n\u001b[1;32m     51\u001b[0m         min_shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     )\n\u001b[1;32m     56\u001b[0m ]\n\u001b[1;32m     57\u001b[0m enabled_precisions \u001b[38;5;241m=\u001b[39m {torch\u001b[38;5;241m.\u001b[39mfloat, torch\u001b[38;5;241m.\u001b[39mhalf}  \u001b[38;5;66;03m# Run with fp16\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m trt_ts_module \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_tensorrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_fx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menabled_precisions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menabled_precisions\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m input_data \u001b[38;5;241m=\u001b[39m input_data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mhalf()\n\u001b[1;32m     64\u001b[0m result \u001b[38;5;241m=\u001b[39m trt_ts_module(input_data)\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/torch_tensorrt/_compile.py:125\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(module, ir, inputs, enabled_precisions, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m         logging\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m    121\u001b[0m             logging\u001b[38;5;241m.\u001b[39mLevel\u001b[38;5;241m.\u001b[39mInfo,\n\u001b[1;32m    122\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule was provided as a torch.nn.Module, trying to script the module with torch.jit.script. In the event of a failure please preconvert your module to TorchScript\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    123\u001b[0m         )\n\u001b[1;32m    124\u001b[0m         ts_mod \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mscript(module)\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_tensorrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mts_mod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menabled_precisions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menabled_precisions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m target_ir \u001b[38;5;241m==\u001b[39m _IRType\u001b[38;5;241m.\u001b[39mfx:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    130\u001b[0m         torch\u001b[38;5;241m.\u001b[39mfloat16 \u001b[38;5;129;01min\u001b[39;00m enabled_precisions\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m torch_tensorrt\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mhalf \u001b[38;5;129;01min\u001b[39;00m enabled_precisions\n\u001b[1;32m    132\u001b[0m     ):\n",
      "File \u001b[0;32m~/anaconda3/envs/aicrowd-product/lib/python3.8/site-packages/torch_tensorrt/ts/_compiler.py:136\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(module, inputs, input_signature, device, disable_tf32, sparse_weights, enabled_precisions, refit, debug, capability, num_avg_timing_iters, workspace_size, dla_sram_size, dla_local_dram_size, dla_global_dram_size, calibrator, truncate_long_and_double, require_full_compilation, min_block_size, torch_executed_ops, torch_executed_modules)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequire_full_compilation is enabled however the list of modules and ops to run in torch is not empty. Found: torch_executed_ops: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch_executed_ops\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, torch_executed_modules: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch_executed_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m spec \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: inputs,\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_signature\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_signature,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m     },\n\u001b[1;32m    134\u001b[0m }\n\u001b[0;32m--> 136\u001b[0m compiled_cpp_mod \u001b[38;5;241m=\u001b[39m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_parse_compile_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m compiled_module \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_recursive\u001b[38;5;241m.\u001b[39mwrap_cpp_module(compiled_cpp_mod)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_module\n",
      "\u001b[0;31mRuntimeError\u001b[0m: outputs_[i]->uses().empty() INTERNAL ASSERT FAILED at \"../torch/csrc/jit/ir/ir.cpp\":1309, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_tensorrt\n",
    "\n",
    "import open_clip\n",
    "import utilities\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Head_Slim(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Head_Slim, self).__init__()\n",
    "\n",
    "        self.emb = nn.Linear(hidden_size, 512, bias=False)\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout((i+1)*.1) for i in range(5)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x_1 = self.emb(self.dropouts[0](x))\n",
    "        x_2 = self.emb(self.dropouts[1](x))\n",
    "        x_3 = self.emb(self.dropouts[2](x))\n",
    "        x_4 = self.emb(self.dropouts[3](x))\n",
    "        x_5 = self.emb(self.dropouts[4](x))\n",
    "\n",
    "        return F.normalize((x_1 + x_2 + x_3 + x_4 + x_5) / 5.0)\n",
    "\n",
    "class Model_Slim(nn.Module):\n",
    "    def __init__(self, vit_backbone):\n",
    "        super(Model_Slim, self).__init__()\n",
    "        self.encoder = vit_backbone.visual\n",
    "        self.head = Head_Slim(1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.head(x)\n",
    "\n",
    "    \n",
    "vit_backbone, _, _ = open_clip.create_model_and_transforms('ViT-H-14', None)\n",
    "model_fx = Model_Slim(vit_backbone)\n",
    "# model_fx.load_state_dict(torch.load(\"./my_experiments/vit_h_224_products-10k/model_best_epoch_0_mAP3_0.53_slim.pt\"))\n",
    "model_fx.eval()\n",
    "model_fx.half()\n",
    "# /\n",
    "\n",
    "input_data = torch.randn(1, 3, 224, 224, dtype=torch.float16)\n",
    "input_data = input_data.to('cuda')\n",
    "# input_data.is_cuda\n",
    "\n",
    "inputs = [\n",
    "    torch_tensorrt.Input(\n",
    "        min_shape=[1, 3, 224, 224],\n",
    "        opt_shape=[1, 3, 224, 224],\n",
    "        max_shape=[512, 3,224, 224],\n",
    "        dtype=torch.half,\n",
    "    )\n",
    "]\n",
    "enabled_precisions = {torch.float, torch.half}  # Run with fp16\n",
    "\n",
    "trt_ts_module = torch_tensorrt.compile(\n",
    "    model_fx, inputs=inputs, enabled_precisions=enabled_precisions\n",
    ")\n",
    "\n",
    "input_data = input_data.to(\"cuda\").half()\n",
    "result = trt_ts_module(input_data)\n",
    "torch.jit.save(trt_ts_module, \"trt_ts_module.ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f4cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import utilities\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import timm\n",
    "import math\n",
    "from transformers import (get_linear_schedule_with_warmup, \n",
    "                          get_cosine_schedule_with_warmup, \n",
    "                          get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "                          get_constant_schedule_with_warmup)\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import random\n",
    "import gc\n",
    "import transformers\n",
    "from transformers import CLIPProcessor, CLIPVisionModel,  CLIPVisionConfig\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from pytorch_metric_learning import losses\n",
    "import open_clip\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import torch.onnx\n",
    "\n",
    "class Head_Slim(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Head_Slim, self).__init__()\n",
    "\n",
    "        self.emb = nn.Linear(hidden_size, 512, bias=False)\n",
    "        self.dropout = utilities.Multisample_Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.dropout(x, self.emb)\n",
    "\n",
    "        return F.normalize(embeddings)\n",
    "\n",
    "class Model_Slim(nn.Module):\n",
    "    def __init__(self, vit_backbone):\n",
    "        super(Model_Slim, self).__init__()\n",
    "        self.encoder = vit_backbone.visual\n",
    "        self.head = Head_Slim(1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.head(x)\n",
    "\n",
    "vit_backbone, _, _ = open_clip.create_model_and_transforms('ViT-H-14', None)\n",
    "model = Model_Slim(vit_backbone)\n",
    "model.load_state_dict(torch.load(\"./my_experiments/vit_h_224_products-10k/model_best_epoch_0_mAP3_0.53_slim.pt\"))\n",
    "model.eval()\n",
    "model.cuda()\n",
    "# model.half()\n",
    "\n",
    "input = torch.randn(1, 3, 224, 224, requires_grad=False, device='cuda')\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    input, \n",
    "    \"./my_experiments/dmy/model_best_epoch_0_mAP3_0.53_slim.onnx\", \n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    verbose=True,\n",
    "    export_params=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "58995ac93f621c7301292278c5e7bacfcfe4a6e6e3b571fc0ea7acaa2147e276"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
